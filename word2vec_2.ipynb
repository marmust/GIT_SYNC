{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from tqdm import tqdm\n",
    "import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset settings\n",
    "dataset_path = fr\"./datasets/ultra_train.txt\"\n",
    "\n",
    "chunk_size = 1024# * 8 * 8\n",
    "corpus_size = 4\n",
    "\n",
    "# model hyperparams\n",
    "vector_size = 28 * 28   # Dimensionality of the word vectors\n",
    "window = 10             # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 2           # Ignores all words with total frequency lower than this\n",
    "workers = 1             # Number of worker threads to train the model\n",
    "sg = 0                  # Training algorithm: 1 for skip-gram; 0 for CBOW\n",
    "hs = 0                  # If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "negative = 28           # If > 0, negative sampling will be used. The int for negative specifies how many \"noise words\" should be drawn\n",
    "epochs = 8              # Number of iterations (epochs) over the corpus\n",
    "alpha = 0.025           # The initial learning rate\n",
    "min_alpha = 0.0001      # The minimum learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "#model = Word2Vec(\n",
    "#    vector_size=vector_size,\n",
    "#    window=window,\n",
    "#    min_count=min_count,\n",
    "#    workers=workers,\n",
    "#    sg=sg,\n",
    "#    hs=hs,\n",
    "#    negative=negative,\n",
    "#    alpha=alpha,\n",
    "#    min_alpha=min_alpha\n",
    "#)\n",
    "\n",
    "model = Word2Vec.load(fr\"./embedding_models/space_split_tokenized_28x28_checkp1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_corpus:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.size = 0\n",
    "        self.corpus_to_size = False\n",
    "        self.all_chunks_to_size = True\n",
    "    \n",
    "    def add_chunk(self, chunk):\n",
    "        self.corpus.append(chunk)\n",
    "        \n",
    "        self.size += 1\n",
    "        self.corpus_to_size = self.size == corpus_size\n",
    "        self.all_chunks_to_size = self.all_chunks_to_size and \"\".join(chunk) == chunk_size\n",
    "    \n",
    "    def corpus_ok(self):\n",
    "        return self.corpus_to_size and self.all_chunks_to_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for chunk in self.corpus:\n",
    "            yield chunk\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(read_start: int, chunk_size: int=chunk_size, path: str=dataset_path) -> tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Reads a certain number of characters from a file starting at the specified position.\n",
    "    \n",
    "    Args:\n",
    "        read_start (int): The position in the file to start reading from.\n",
    "        chunk_size (int, optional): The number of characters to read. Defaults to `chunk_size`.\n",
    "        path (str, optional): The path to the dataset text file. Defaults to `dataset_path`.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[str, bool]: A tuple containing the loaded chunk and a boolean indicating if EOF is hit.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        file.seek(read_start)\n",
    "        chunk = file.read(chunk_size)\n",
    "        \n",
    "        # eof hit check\n",
    "        if not chunk:\n",
    "            return \"\", True\n",
    "        \n",
    "        return chunk, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(read_start: int, corpus_size: int=corpus_size, chunk_size: int=chunk_size, path: str=dataset_path) -> tuple[training_corpus, bool]:\n",
    "    corpus = training_corpus()\n",
    "    \n",
    "    for _ in range(corpus_size):\n",
    "        chunk, eof = read_chunk(read_start, chunk_size, path)\n",
    "        \n",
    "        # check eof\n",
    "        if eof:\n",
    "            return corpus, True\n",
    "        \n",
    "        corpus.add_chunk(tokenizer.tokenize_segment(chunk))\n",
    "        \n",
    "        read_start += chunk_size\n",
    "        \n",
    "    return corpus, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_corpus(model: Word2Vec, corpus: training_corpus, first=False):\n",
    "    \"\"\"\n",
    "    Train a Word2Vec model on a given corpus chunk.\n",
    "\n",
    "    Args:\n",
    "        model (Word2Vec): The Word2Vec model to train.\n",
    "        corpus (list): A list of tokenized sentences to train on.\n",
    "        first (bool): If True, build the vocabulary from the corpus. Use this only for the first chunk.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if first and False:\n",
    "        model.build_vocab(corpus.corpus)\n",
    "        total_examples = model.corpus_count\n",
    "    else:\n",
    "        # Update the vocabulary with new words from the corpus\n",
    "        model.build_vocab(corpus.corpus, update=True)\n",
    "        total_examples = model.corpus_count\n",
    "    \n",
    "    model.train(corpus.corpus, total_examples=total_examples, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(model, dataset_path, train_corpuses):\n",
    "    read_start = 0\n",
    "    corpus = training_corpus()\n",
    "    \n",
    "    # first train\n",
    "    corpus, eof = build_corpus(read_start, corpus_size, chunk_size, dataset_path)\n",
    "    train_corpus(model, corpus, first=True)\n",
    "    \n",
    "    for _ in tqdm(range(train_corpuses)):\n",
    "        corpus, eof = build_corpus(read_start, corpus_size, chunk_size, dataset_path)\n",
    "        \n",
    "        if eof:\n",
    "            break\n",
    "        \n",
    "        if _ > 5070 or False:\n",
    "            train_corpus(model, corpus)\n",
    "        \n",
    "        read_start += corpus_size * chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/192000 [00:00<55:18, 57.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 5070/192000 [17:52<10:59:20,  4.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfull_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mfull_train\u001b[0;34m(model, dataset_path, train_corpuses)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _ \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m85858\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtrain_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m read_start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m corpus_size \u001b[38;5;241m*\u001b[39m chunk_size\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mtrain_corpus\u001b[0;34m(model, corpus, first)\u001b[0m\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus\u001b[38;5;241m.\u001b[39mcorpus, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m     total_examples \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcorpus_count\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/gensim/models/word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[1;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[1;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/gensim/models/word2vec.py:1432\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m workers:\n\u001b[1;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m-> 1432\u001b[0m     \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch_progress(\n\u001b[1;32m   1435\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1436\u001b[0m     total_words\u001b[38;5;241m=\u001b[39mtotal_words, report_delay\u001b[38;5;241m=\u001b[39mreport_delay, is_corpus_file_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1437\u001b[0m )\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/threading.py:969\u001b[0m, in \u001b[0;36mThread.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m _limbo[\u001b[38;5;28mself\u001b[39m]\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_started\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_train(model, dataset_path, 6000 * 16 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark:\n",
    "# 40 min train   /   580 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('always', 0.6252385973930359),\n",
       " ('still', 0.5941474437713623),\n",
       " (\"there's\", 0.5674745440483093),\n",
       " ('much', 0.5590306520462036),\n",
       " ('so', 0.547294557094574),\n",
       " ('never', 0.5472665429115295),\n",
       " ('worth', 0.5424566864967346),\n",
       " ('Is', 0.5310291647911072),\n",
       " (\"didn't\", 0.5303480625152588),\n",
       " ('realize', 0.5266621708869934)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fr\"./embedding_models/space_split_tokenized_28x28_checkp2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24262"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
