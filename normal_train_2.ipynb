{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   CONFIGURATION   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pickle\n",
    "import threading\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import tokenizer\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = fr\"./embedding_models/b4cksh0t5_checkp3.model\"\n",
    "embeddings_model = Word2Vec.load(model_file)\n",
    "\n",
    "vector_size = embeddings_model.vector_size        # aka embedding dim \n",
    "\n",
    "# neural net settings\n",
    "context_length = 32                               # tokens to consider\n",
    "attn_heads = 8                                    # num attention heads per mechanism (per transformer block)\n",
    "dropout_prob = 0.0                                # 0.0 ---> everything normal   |   1.0 ---> everything is random\n",
    "\n",
    "# dataset\n",
    "train_dataset_path = fr\"./datasets/ultra_train.txt\"\n",
    "test_dataset_path = fr\"./datasets/ultra_test.txt\"\n",
    "\n",
    "examples_train = 64 * 8 * 8 * 8 * 8 * 8 * 8 * 8\n",
    "examples_test = 64 * 8 * 8\n",
    "\n",
    "# training\n",
    "train_epochs = 2\n",
    "\n",
    "start_lr = 0.00001\n",
    "final_lr = 0.000001\n",
    "\n",
    "class CosineMSELoss(nn.Module):\n",
    "    def __init__(self, blend=0.2, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Custom loss function that combines Cosine Similarity Loss and MSE Loss.\n",
    "        \n",
    "        Parameters:\n",
    "        - blend: A value between 0 and 1, where 0 means only cosine loss and 1 means only MSE loss.\n",
    "        - reduction: Specifies the reduction to apply to the output ('mean', 'sum', 'none').\n",
    "        \"\"\"\n",
    "        super(CosineMSELoss, self).__init__()\n",
    "        self.blend = blend\n",
    "        self.cosine_loss = nn.CosineEmbeddingLoss(reduction=reduction)\n",
    "        self.mse_loss = nn.MSELoss(reduction=reduction)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        # Cosine similarity expects labels of 1 or -1, we assume positive similarity, so labels = 1\n",
    "        cosine_similarity_label = torch.ones(input.size(0), device=input.device)\n",
    "        cosine_loss = self.cosine_loss(input, target, cosine_similarity_label)\n",
    "        mse_loss = self.mse_loss(input, target)\n",
    "        \n",
    "        # Blend the two losses based on the 'blend' parameter\n",
    "        loss = self.blend * mse_loss + (1 - self.blend) * cosine_loss\n",
    "        \n",
    "        return loss\n",
    "\n",
    "loss = CosineMSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "\n",
    "train_batch_size = int(128 * 4 * 2)\n",
    "test_batch_size = int(128 * 4 * 2)\n",
    "test_loop_batch = 24\n",
    "\n",
    "# plots\n",
    "plot_graphs = True\n",
    "plot_batches = 4 * 4 * 2\n",
    "background_color = \"#0c0220\"\n",
    "text_color =       \"#06f8eb\"\n",
    "axis_color =       \"#06f8eb\"\n",
    "train_loss_color = \"#fca927\"\n",
    "test_loss_color =  \"#fca927\"\n",
    "lr_color =         \"#fca927\"\n",
    "\n",
    "# pytorch\n",
    "run_device = torch.device(\"cuda\")\n",
    "storage_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   NEURAL NET ARCHITECTURE   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_tanh_smart(nn.Module):\n",
    "    def __init__(self, leaky_range=(0, 3), squishy_range=(0, 3)):\n",
    "        super(leaky_tanh_smart, self).__init__()\n",
    "        # register leakyness and squishyness as trainable parameters\n",
    "        self.leakyness = nn.Parameter(torch.rand(1, dtype=torch.float32) * (leaky_range[1] - leaky_range[0]) + leaky_range[0])\n",
    "        self.squishyness = nn.Parameter(torch.rand(1, dtype=torch.float32) * (squishy_range[1] - squishy_range[0]) + squishy_range[0])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        applies the leaky tanh activation function over the input tensor x.\\n\n",
    "        for more info on leaky tanh and its parameters go to: https://www.desmos.com/calculator/kpzsfbtqww\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): tensor over which to apply activation function.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: returns x after function applied, keeps the same shape.\n",
    "        \"\"\"\n",
    "        \n",
    "        return F.tanh(x * self.squishyness) + self.leakyness * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_mech(nn.Module):\n",
    "    def __init__(self, vector_size=vector_size, attn_heads=attn_heads):\n",
    "        super(attention_mech, self).__init__()\n",
    "        # MultiheadAttention module\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=vector_size, num_heads=attn_heads)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(vector_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Prepare for multi-head attention (transpose to (sentence_len, batch_size, embedding_dim))\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Create causal mask\n",
    "        seq_len = x.size(0)\n",
    "        causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=x.device), diagonal=1).bool()\n",
    "        \n",
    "        # Apply multi-head attention with the causal mask\n",
    "        attn_output, attn_weights = self.multihead_attn(x, x, x, attn_mask=causal_mask)\n",
    "        \n",
    "        # Apply layer normalization to the attention output\n",
    "        attn_output = self.norm(attn_output)\n",
    "        \n",
    "        # Transpose back to (batch_size, sentence_len, embedding_dim)\n",
    "        output = attn_output.transpose(0, 1)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block_widthwise(nn.Module):\n",
    "    def __init__(self, vector_size=vector_size, dropout_prob=dropout_prob):\n",
    "        super(transformer_block_widthwise, self).__init__()\n",
    "        \n",
    "        self.activ_func1 = leaky_tanh_smart()\n",
    "        \n",
    "        self.attn1 = attention_mech()\n",
    "        self.attn2 = attention_mech()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.fc_widthwise = nn.Linear(vector_size, vector_size)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(vector_size)\n",
    "        self.norm2 = nn.LayerNorm(vector_size)\n",
    "        self.norm3 = nn.LayerNorm(vector_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.norm1(x + self.attn1(x)[0])\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm2(x + self.activ_func1(self.fc_widthwise(x)))\n",
    "        x = self.norm3(x + self.attn2(x)[0])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block_lengthwise(nn.Module):\n",
    "    def __init__(self, vector_size=vector_size, dropout_prob=dropout_prob):\n",
    "        super(transformer_block_lengthwise, self).__init__()\n",
    "        \n",
    "        self.activ_func1 = leaky_tanh_smart()\n",
    "        \n",
    "        self.attn1 = attention_mech()\n",
    "        self.attn2 = attention_mech()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.fc_lengthwise = nn.Linear(context_length, context_length)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(vector_size)\n",
    "        self.norm2 = nn.LayerNorm(vector_size)\n",
    "        self.norm3 = nn.LayerNorm(vector_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.norm1(self.attn1(x)[0])\n",
    "        x = self.dropout(x)\n",
    "        x = x + self.norm2(self.activ_func1(self.fc_lengthwise(x.permute(0, 2, 1))).permute(0, 2, 1))\n",
    "        x = x + self.norm2(self.attn2(x)[0])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(output_head, self).__init__()\n",
    "        \n",
    "        self.af = leaky_tanh_smart()\n",
    "        \n",
    "        self.fc1 = nn.Linear(context_length * vector_size, vector_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.af(self.fc1(x.flatten(start_dim=1)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(REAN, self).__init__()\n",
    "        \n",
    "        self.tblock1 = transformer_block_lengthwise()\n",
    "        self.tblock2 = transformer_block_widthwise()\n",
    "        self.tblock3 = transformer_block_lengthwise()\n",
    "        self.tblock4 = transformer_block_widthwise()\n",
    "\n",
    "        self.out_head = output_head()\n",
    "\n",
    "    def forward(self, segment: torch.Tensor) -> torch.Tensor:\n",
    "        ###                  INPUT                 ###\n",
    "        #    (batches, context_len, vector_size)\n",
    "        #                      ↓\n",
    "        \n",
    "        segment = self.tblock1(segment)\n",
    "        segment = self.tblock2(segment)\n",
    "        segment = self.tblock3(segment)\n",
    "        segment = self.tblock4(segment)\n",
    "        \n",
    "        segment = self.out_head(segment)\n",
    "        \n",
    "        #                      ↓\n",
    "        #            (batches, vector_size)\n",
    "        ###                 OUTPUT                 ###\n",
    "        \n",
    "        return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   BUILD NET & DEPENDENCIES   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural net weight: 0.1514GB\n"
     ]
    }
   ],
   "source": [
    "net = REAN()\n",
    "\n",
    "net.to(run_device)\n",
    "\n",
    "optimizer = optimizer(net.parameters(), lr=start_lr)\n",
    "scheduler = scheduler(optimizer, T_max=train_epochs, eta_min=final_lr)\n",
    "\n",
    "print(f\"neural net weight: {sum(param.numel() * param.element_size() for param in net.parameters()) / (1024 ** 3):.4f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   UTIL FUNCS   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_segment(segment: list[str], model: Word2Vec=embeddings_model, default: int = 0, used_device=storage_device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    encodes all words in a given list to corresponding vectors in given model.\n",
    "    words not found in the model will be given a vector with \"default\" value\n",
    "    \n",
    "    Args:\n",
    "        sentence (list): list of strings (tokenized sentence)\n",
    "        model (Word2Vec): model to use when encoding\n",
    "        default (int): fill vector with this value if word is not found in model\n",
    "    \n",
    "    Returns:\n",
    "        np.array: 2d array with dim1 = len(sentence) and dim2 = model.vector_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate inital array with default values\n",
    "    vectorized = np.ones((len(segment), model.vector_size)) * default\n",
    "    \n",
    "    # loop over every word in list\n",
    "    for current_word, current_word_idx in zip(segment, range(len(segment))):\n",
    "        # only add correct values if word is in model, otherwise leave as default\n",
    "        if current_word in model.wv:\n",
    "            # the try except block is needed because (current_word in model.wv) sometimes gives a false positive... yeah gensim\n",
    "            try:\n",
    "                vectorized[current_word_idx] = model.wv.get_vector(current_word, norm=False)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    vectorized = torch.tensor(vectorized, dtype=torch.float32, device=used_device)\n",
    "    \n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devectorize_segment(vectorized_segment: torch.Tensor, model: Word2Vec=embeddings_model, not_in_vocab_token=\"[NIV]\", NIV_threshold=0.01) -> list:\n",
    "    \"\"\"\n",
    "    decodes vectors into nearest word found in model, if no near words found, adds a not in vocab token\n",
    "    \n",
    "    Args:\n",
    "        vectorized_sentence (np.array): 2d arrat with vectors of words to be decoded\n",
    "        model (Word2Vec): model to use when decoding\n",
    "    \n",
    "    Returns:\n",
    "        list: list of strings (words) whos vectors most closely match those provided\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # make sure vectors are ready to be processed\n",
    "    vectorized_segment = vectorized_segment.cpu().numpy()\n",
    "    \n",
    "    # go over all words and find closest match in model\n",
    "    for current_word in vectorized_segment:\n",
    "        similarities = model.wv.similar_by_vector(current_word)\n",
    "        \n",
    "        # check if its not a bullshit vector\n",
    "        if similarities[0][1] > NIV_threshold:\n",
    "            result.append(similarities[0][0])\n",
    "        else:\n",
    "            result.append(not_in_vocab_token)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(suspected_tensor: torch.tensor, target_length: int, default: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pads or truncates a given tensor along dim 0 to target_length with \"default\" as padding\n",
    "    \n",
    "    Args:\n",
    "        suspected_tensor (torch.tensor): tensor to pad or truncate\n",
    "        target_length (int): target length of tensor\n",
    "        default (int): value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(suspected_tensor) < target_length:\n",
    "        # pad\n",
    "        suspected_tensor = torch.cat((torch.ones(target_length - len(suspected_tensor), suspected_tensor.shape[1], dtype=torch.float32, device=suspected_tensor.device) * default, suspected_tensor))\n",
    "    else:\n",
    "        # truncate\n",
    "        suspected_tensor = suspected_tensor[-target_length:]\n",
    "    \n",
    "    return suspected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segment_for_net(segment: list[str], length: int=context_length, used_device: torch.DeviceObjType=storage_device):\n",
    "    \"\"\"\n",
    "    function to take a sentence, and do everything to make it possible to input into the net\n",
    "    \n",
    "    Args:\n",
    "        segment (list[str]): a list of tokens (ideally from the tokenizer) of a sentence / text\n",
    "        length (int): the number of tokens to which pad or truncate to. for correct operation: keep at the net's context length\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: tokenized segment in the correct length\n",
    "    \"\"\"\n",
    "    \n",
    "    # turn into embedding vectors\n",
    "    vectorized = vectorize_segment(segment, used_device=used_device)\n",
    "    \n",
    "    # trim / add into length\n",
    "    trimmed = pad_or_truncate(vectorized, length)\n",
    "    \n",
    "    # add fake batch dimension\n",
    "    batched = trimmed.unsqueeze(0)\n",
    "    \n",
    "    return batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(segment: list[str], net: REAN=net):\n",
    "    # turn tokenized text into net's format\n",
    "    prepared_segment = prepare_segment_for_net(segment, used_device=next(net.parameters()).device)\n",
    "    \n",
    "    # run net\n",
    "    prediction_vector = net(prepared_segment).detach()\n",
    "    \n",
    "    # turn vector back into token\n",
    "    predicted_token = devectorize_segment(prediction_vector)\n",
    "    \n",
    "    return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(segment: list[str], num_tokens: int, net: REAN=net):\n",
    "    result = segment.copy()\n",
    "    \n",
    "    for _ in tqdm(range(num_tokens)):\n",
    "        result += predict_word(result, net=net)\n",
    "    \n",
    "    return result[len(segment):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   BUILD DATASET   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN_dataset(Dataset):\n",
    "    def pull_tokens(self, start_read_idx: int, requested_num_tokens: int):\n",
    "        \"\"\"\n",
    "        function returns a requested number of tokens from the dataset file, starting at APPROXIMATLY the start_read_idx token.\\n\n",
    "        attempts to return full words as much as possible, example:\\n\n",
    "        NO:    this | is | a | sen (tence)\\n\n",
    "        YES:   this | is | a | sentence\n",
    "        \n",
    "        Args:\n",
    "            start_read_idx (int): the APPROXIMATE token at which to start the reading (determined from the avarage token length in the tokenizer vocab)\n",
    "            requested_num_tokens (int): how many tokens to return\n",
    "        \n",
    "        Returns:\n",
    "            tokenized text (list of str): the tokens of the dataset from start_read_idx to start_read_idx + requested_num_tokens\n",
    "            is EOF hit (bool): if the requested args were outside of the dataset's range\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(self.path, errors=\"ignore\") as self.dataset:\n",
    "            self.dataset.seek(start_read_idx * tokenizer.average_token_length)\n",
    "            \n",
    "            # get an initial estimate to what text we will actually need\n",
    "            self.buffer = self.dataset.read(requested_num_tokens * tokenizer.average_token_length)\n",
    "            self.tokenized_buffer = tokenizer.tokenize_segment(self.buffer)\n",
    "            self.current_num_tokens = len(self.tokenized_buffer)\n",
    "            \n",
    "            # if the estimate we took is too small, we enlarge it character by character until its perfect\n",
    "            while self.current_num_tokens < requested_num_tokens + 1:\n",
    "                self.next_char = self.dataset.read(1)  # seperate variable to check EOF\n",
    "                \n",
    "                # check eof\n",
    "                if not self.next_char:\n",
    "                    print(\"pull_tokens(): eof was hit\")\n",
    "                    return self.tokenized_buffer[-requested_num_tokens - 1:][:-1], True\n",
    "                \n",
    "                self.buffer += self.next_char\n",
    "                \n",
    "                self.tokenized_buffer = tokenizer.tokenize_segment(self.buffer)\n",
    "                self.current_num_tokens = len(self.tokenized_buffer)\n",
    "        \n",
    "        # regardless of if the estimate is too long / short, return theproper amount of tokens, with the end snipped of, because it might be a half token\n",
    "        return self.tokenized_buffer[-requested_num_tokens - 1:][:-1], False\n",
    "    \n",
    "    def construct_example(self, start_read_idx: int):\n",
    "        \"\"\"\n",
    "        function to make a full datapoint, can be used as raw return for __getitem__()\n",
    "        \n",
    "        Args:\n",
    "            start_read_idx (int): at which token to start making the example\n",
    "        \n",
    "        Returns:\n",
    "            tokenized text (list of str): the tokens of the dataset from start_read_idx to start_read_idx + self.context_length\n",
    "        \"\"\"\n",
    "        \n",
    "        # pull neccesary amount of tokens for question / input and answer / output\n",
    "        self.tokens, _ = self.pull_tokens(start_read_idx, self.context_length + 1)\n",
    "        \n",
    "        # encode the tokens to vectors (aka embeddings)\n",
    "        self.vectorized_tokens = prepare_segment_for_net(self.tokens, length=self.context_length + 1).squeeze(0)\n",
    "        \n",
    "        # split into network input and expected output\n",
    "        self.question = self.vectorized_tokens[:-1] # everythinbg up to last word\n",
    "        self.answer = self.vectorized_tokens[-1] # last word itself\n",
    "        \n",
    "        return self.question, self.answer\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        function to read thru the whole dataset, and report how many examples there are / if there are as many as the user requested\n",
    "        \n",
    "        Args:\n",
    "            none, but uses self.num_tokens and self.context_length\n",
    "        \n",
    "        Returns:\n",
    "            returns how many usable examples there are, for __len__()\n",
    "        \"\"\"\n",
    "        \n",
    "        with tqdm(total=self.num_examples, desc=\"Calculating Dataset Size\", unit=\"example\") as pbar:\n",
    "            for self.current_check in range(self.num_examples):\n",
    "                _, self.eof = self.pull_tokens(self.current_check, self.context_length)\n",
    "                \n",
    "                if self.eof:\n",
    "                    print(\"The requested size is bigger than the .txt provided, so the dataset might be smaller than what you expected.\")\n",
    "                    break\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"Requested num_examples: {self.num_examples}\\nActual size found:      {self.current_check - 1}\")\n",
    "        \n",
    "        return self.current_check - 1   # the -1 is just in case\n",
    "    \n",
    "    def __init__(self, path, num_examples, context_length, embeddings_model, verify_dataset_size=True):\n",
    "        # transfer to object wide variables\n",
    "        self.path = path\n",
    "        self.context_length = context_length\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.num_examples = num_examples\n",
    "        \n",
    "        # get the size of the dataset txt file\n",
    "        self.dataset_len = num_examples\n",
    "        \n",
    "        if verify_dataset_size:\n",
    "            self.dataset_len = self.get_size()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.construct_example(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = REAN_dataset(train_dataset_path, examples_train, context_length, embeddings_model, verify_dataset_size=False)\n",
    "test_dataset = REAN_dataset(test_dataset_path, examples_test, context_length, embeddings_model, verify_dataset_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please validate dataset: does this look correct?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"please validate dataset: does this look correct?\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    rnd_offset = random.randint(2097152 - 10000, 2097152 - 10000)\n",
    "    \n",
    "    for idx in range(0):\n",
    "        print(f\"{tokenizer.detokenize_segment(devectorize_segment(train_dataset[idx + rnd_offset][0].detach(), embeddings_model))}   --->   {tokenizer.detokenize_segment(devectorize_segment(train_dataset[idx + rnd_offset][1].detach().unsqueeze(0), embeddings_model))}\".replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if num_workers arg is used\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_graph = []\n",
    "test_loss_graph = []\n",
    "learning_rate_graph = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:14<00:14, 14.34s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 241.38 MiB is free. Including non-PyTorch memory, this process has 21.74 GiB memory in use. Of the allocated memory 21.22 GiB is allocated by PyTorch, and 233.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(run_device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# train batch\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m train_loss_value \u001b[38;5;241m=\u001b[39m loss(train_outputs, target)\n\u001b[1;32m     18\u001b[0m train_loss_value\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mREAN.forward\u001b[0;34m(self, segment)\u001b[0m\n\u001b[1;32m     18\u001b[0m segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtblock2(segment)\n\u001b[1;32m     19\u001b[0m segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtblock3(segment)\n\u001b[0;32m---> 20\u001b[0m segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtblock4\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(segment)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#                      ↓\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#            (batches, vector_size)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m###                 OUTPUT                 ###\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mtransformer_block_widthwise.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(x)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiv_func1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_widthwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2(x)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 241.38 MiB is free. Including non-PyTorch memory, this process has 21.74 GiB memory in use. Of the allocated memory 21.22 GiB is allocated by PyTorch, and 233.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "with tqdm(total=train_epochs) as pbar:\n",
    "    batch = 0\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # training loop\n",
    "        for current_segment, target in train_loader:\n",
    "            batch += 1\n",
    "            \n",
    "            # move batch to gpu\n",
    "            current_segment = current_segment.to(run_device)\n",
    "            target = target.to(run_device)\n",
    "            \n",
    "            # train batch\n",
    "            train_outputs = net(current_segment)\n",
    "            train_loss_value = loss(train_outputs, target)\n",
    "            train_loss_value.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # collect performance metrics\n",
    "            train_loss_graph.append(train_loss_value.item())\n",
    "            \n",
    "            # plot everything\n",
    "            if batch % plot_batches == 0:\n",
    "                if plot_graphs:\n",
    "                    clear_output(wait=True)\n",
    "                    \n",
    "                    pbar.refresh()\n",
    "                    \n",
    "                    plt.figure(figsize=(15, 5), facecolor=background_color)\n",
    "\n",
    "                    # Plot training loss\n",
    "                    ax1 = plt.subplot(1, 3, 1, facecolor=background_color)\n",
    "                    plt.plot(train_loss_graph, label='Train Loss', color=train_loss_color)\n",
    "                    plt.xlabel('Epoch', color=text_color)\n",
    "                    plt.ylabel('Loss', color=text_color)\n",
    "                    plt.legend(facecolor=background_color, edgecolor='none', labelcolor=text_color)\n",
    "                    # Set spines to white\n",
    "                    ax1.spines['top'].set_color(axis_color)\n",
    "                    ax1.spines['bottom'].set_color(axis_color)\n",
    "                    ax1.spines['left'].set_color(axis_color)\n",
    "                    ax1.spines['right'].set_color(axis_color)\n",
    "                    # Set tick colors to white\n",
    "                    ax1.tick_params(axis='x', colors=axis_color)\n",
    "                    ax1.tick_params(axis='y', colors=axis_color)\n",
    "\n",
    "                    # Plot testing loss\n",
    "                    ax2 = plt.subplot(1, 3, 2, facecolor=background_color)\n",
    "                    plt.plot(range(0, len(test_loss_graph) * 3, 3), test_loss_graph, label='Test Loss', color=test_loss_color)\n",
    "                    plt.xlabel('Epoch', color=text_color)\n",
    "                    plt.ylabel('Loss', color=text_color)\n",
    "                    plt.legend(facecolor=background_color, edgecolor='none', labelcolor=text_color)\n",
    "                    # Set spines to white\n",
    "                    ax2.spines['top'].set_color(axis_color)\n",
    "                    ax2.spines['bottom'].set_color(axis_color)\n",
    "                    ax2.spines['left'].set_color(axis_color)\n",
    "                    ax2.spines['right'].set_color(axis_color)\n",
    "                    # Set tick colors to white\n",
    "                    ax2.tick_params(axis='x', colors=axis_color)\n",
    "                    ax2.tick_params(axis='y', colors=axis_color)\n",
    "\n",
    "                    # Plot learning rate\n",
    "                    ax3 = plt.subplot(1, 3, 3, facecolor=background_color)\n",
    "                    plt.plot(learning_rate_graph, label='Learning Rate', color=lr_color)\n",
    "                    plt.xlabel('Epoch', color=text_color)\n",
    "                    plt.ylabel('Learning Rate', color=text_color)\n",
    "                    plt.legend(facecolor=background_color, edgecolor='none', labelcolor=text_color)\n",
    "                    # Set spines to white\n",
    "                    ax3.spines['top'].set_color(axis_color)\n",
    "                    ax3.spines['bottom'].set_color(axis_color)\n",
    "                    ax3.spines['left'].set_color(axis_color)\n",
    "                    ax3.spines['right'].set_color(axis_color)\n",
    "                    # Set tick colors to white\n",
    "                    ax3.tick_params(axis='x', colors=axis_color)\n",
    "                    ax3.tick_params(axis='y', colors=axis_color)\n",
    "\n",
    "                    # Adjust layout and show plot\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "            # eval loop\n",
    "            if batch % test_loop_batch == 0:\n",
    "                with torch.no_grad():\n",
    "                    for test_current_segment, test_target in test_loader:\n",
    "                        # move batch to gpu\n",
    "                        test_current_segment = test_current_segment.to(run_device)\n",
    "                        test_target = test_target.to(run_device)\n",
    "                        \n",
    "                        # run test\n",
    "                        test_outputs = net(test_current_segment)\n",
    "                        test_loss_value = loss(test_outputs, test_target)\n",
    "                        \n",
    "                        # collect performance metrics\n",
    "                        test_loss_graph.append(test_loss_value.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # collect perforamce metrics\n",
    "        learning_rate_graph.append(optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:04<00:00, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The s s Grad Chhau ZhengzhouTWICE Ecoterra picnic\n",
      ": The macaw HoganHanekektm mozzarella\n",
      "Mirzakhani and annapurna and and Mirzakhani the the of of the the of of the the the of the the the of the the the the the the the the the of the the s of the the the of s the the of the the the the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.detokenize_segment(predict_sequence(tokenizer.tokenize_segment(\"human: how do i bake potatoes? network: \"), 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, './REAN_nets/meth_abuser6969_attn_stack.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
