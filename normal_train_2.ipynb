{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   CONFIGURATION   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pickle\n",
    "import threading\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import tokenizer\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = fr\"./embedding_models/b4cksh0t5_checkp3.model\"\n",
    "embeddings_model = Word2Vec.load(model_file)\n",
    "\n",
    "vector_size = embeddings_model.vector_size        # aka embedding dim \n",
    "\n",
    "# neural net settings\n",
    "context_length = 64                               # tokens to consider\n",
    "attn_heads = 8                                    # num attention heads per mechanism (per transformer block)\n",
    "dropout_prob = 0.0                                # 0.0 ---> everything normal   |   1.0 ---> everything is random\n",
    "\n",
    "# dataset\n",
    "train_dataset_path = fr\"./datasets/ultra_train.txt\"\n",
    "test_dataset_path = fr\"./datasets/ultra_test.txt\"\n",
    "\n",
    "examples_train = 64 * 8 * 8 * 8 * 8 * 8 * 8 * 8 * 8\n",
    "examples_test = 64 * 8 * 8\n",
    "\n",
    "# training\n",
    "train_epochs = 256\n",
    "\n",
    "gradient_accumulation_batches = 1\n",
    "\n",
    "start_lr = 0.0001\n",
    "final_lr = 0.00000001\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Initializes the CombinedLoss module.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha (float): Weighting factor between MSE Loss and Cosine Similarity Loss.\n",
    "                         Must be between 0 and 1. Default is 0.5.\n",
    "                         - alpha = 1.0: Only MSE Loss.\n",
    "                         - alpha = 0.0: Only Cosine Similarity Loss.\n",
    "        \"\"\"\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for combining losses\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Ensure inputs and targets are 2D tensors with shape (batch_size, num_features)\n",
    "        if input.dim() == 1:\n",
    "            input = input.unsqueeze(0)  # Add batch dimension\n",
    "        if target.dim() == 1:\n",
    "            target = target.unsqueeze(0)\n",
    "\n",
    "        # Flatten the input (batch_size, num_features)\n",
    "        input = input.view(input.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "\n",
    "        ### Mean Squared Error Loss ###\n",
    "        # Compute the MSE loss between the input and the target\n",
    "        mse_loss = F.mse_loss(input, target, reduction='mean')\n",
    "\n",
    "        ### Cosine Similarity Loss ###\n",
    "        # Normalize the inputs and targets to unit vectors\n",
    "        input_norm = F.normalize(input, p=2, dim=1)\n",
    "        target_norm = F.normalize(target, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity for each sample in the batch\n",
    "        cosine_sim = torch.sum(input_norm * target_norm, dim=1)\n",
    "\n",
    "        # Cosine similarity loss for each sample: 1 - cosine similarity\n",
    "        cosine_loss = 1 - cosine_sim\n",
    "\n",
    "        # Mean over the batch\n",
    "        cosine_loss = cosine_loss.mean()\n",
    "\n",
    "        ### Combine the Losses ###\n",
    "        # Weighted sum of MSE loss and Cosine Similarity loss\n",
    "        loss = self.alpha * mse_loss + (1 - self.alpha) * cosine_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "loss = nn.MSELoss()#CombinedLoss()\n",
    "optimizer = torch.optim.Adam\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "\n",
    "train_batch_size = int(128 * 4 * 1)\n",
    "test_batch_size = int(128 * 4 * 1)\n",
    "test_loop_epoch = 4\n",
    "\n",
    "# plots\n",
    "plot_graphs = True\n",
    "plot_batches = 4 * 4 * 4\n",
    "background_color = \"#0c0220\"\n",
    "text_color =       \"#06f8eb\"\n",
    "axis_color =       \"#06f8eb\"\n",
    "train_loss_color = \"#fca927\"\n",
    "test_loss_color =  \"#fca927\"\n",
    "lr_color =         \"#fca927\"\n",
    "\n",
    "# pytorch\n",
    "run_device = torch.device(\"cuda\")\n",
    "storage_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   NEURAL NET ARCHITECTURE   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_tanh_smart(nn.Module):\n",
    "    def __init__(self, leaky_range=(0, 3), squishy_range=(0, 3)):\n",
    "        super(leaky_tanh_smart, self).__init__()\n",
    "        # register leakyness and squishyness as trainable parameters\n",
    "        self.leakyness = nn.Parameter(torch.rand(1, dtype=torch.float32) * (leaky_range[1] - leaky_range[0]) + leaky_range[0])\n",
    "        self.squishyness = nn.Parameter(torch.rand(1, dtype=torch.float32) * (squishy_range[1] - squishy_range[0]) + squishy_range[0])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        applies the leaky tanh activation function over the input tensor x.\\n\n",
    "        for more info on leaky tanh and its parameters go to: https://www.desmos.com/calculator/kpzsfbtqww\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): tensor over which to apply activation function.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: returns x after function applied, keeps the same shape.\n",
    "        \"\"\"\n",
    "        \n",
    "        return F.tanh(x * self.squishyness) + self.leakyness * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_mech(nn.Module):\n",
    "    def __init__(self, vector_size=vector_size, attn_heads=attn_heads):\n",
    "        super(attention_mech, self).__init__()\n",
    "        # MultiheadAttention module\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=vector_size, num_heads=attn_heads)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(vector_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Prepare for multi-head attention (transpose to (sentence_len, batch_size, embedding_dim))\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Apply multi-head attention\n",
    "        attn_output, attn_weights = self.multihead_attn(x, x, x)\n",
    "        \n",
    "        # Apply layer normalization to the attention output\n",
    "        attn_output = self.norm(attn_output)\n",
    "        \n",
    "        # Transpose back to (batch_size, sentence_len, embedding_dim)\n",
    "        output = attn_output.transpose(0, 1)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(nn.Module):\n",
    "    def __init__(self, vector_size=vector_size, dropout_prob=dropout_prob):\n",
    "        super(transformer_block, self).__init__()\n",
    "        \n",
    "        self.activ_func1 = leaky_tanh_smart()\n",
    "        \n",
    "        self.attn1 = attention_mech()\n",
    "        \n",
    "        self.fc_lengthwise = nn.Linear(context_length, context_length)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(vector_size)\n",
    "        self.norm2 = nn.LayerNorm(vector_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.norm1(self.attn1(x)[0])\n",
    "        x = self.norm2(self.activ_func1(self.fc_lengthwise(x.permute(0, 2, 1))).permute(0, 2, 1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(output_head, self).__init__()\n",
    "        \n",
    "        self.attn_mech = attention_mech()\n",
    "        \n",
    "        self.fc1 = nn.Linear(context_length * vector_size, vector_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.attn_mech(x)[0]\n",
    "        \n",
    "        x = self.fc1(x.flatten(start_dim=1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(REAN, self).__init__()\n",
    "        \n",
    "        self.tblock1 = transformer_block()\n",
    "        self.tblock2 = transformer_block()\n",
    "        self.tblock3 = transformer_block()\n",
    "        self.tblock4 = transformer_block()\n",
    "\n",
    "        self.out_head = output_head()\n",
    "\n",
    "    def forward(self, segment: torch.Tensor) -> torch.Tensor:\n",
    "        ###                  INPUT                 ###\n",
    "        #    (batches, context_len, vector_size)\n",
    "        #                      ↓\n",
    "        \n",
    "        segment = self.tblock1(segment)\n",
    "        segment = self.tblock2(segment)\n",
    "        segment = self.tblock3(segment)\n",
    "        segment = self.tblock4(segment)\n",
    "        \n",
    "        segment = self.out_head(segment)\n",
    "        \n",
    "        #                      ↓\n",
    "        #            (batches, vector_size)\n",
    "        ###                 OUTPUT                 ###\n",
    "        \n",
    "        return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   BUILD NET & DEPENDENCIES   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural net weight: 0.1925GB\n"
     ]
    }
   ],
   "source": [
    "net = REAN()\n",
    "\n",
    "net.to(run_device)\n",
    "\n",
    "optimizer = optimizer(net.parameters(), lr=start_lr)\n",
    "scheduler = scheduler(optimizer, T_max=train_epochs, eta_min=final_lr)\n",
    "\n",
    "print(f\"neural net weight: {sum(param.numel() * param.element_size() for param in net.parameters()) / (1024 ** 3):.4f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   UTIL FUNCS   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_segment(segment: list[str], model: Word2Vec=embeddings_model, default: int = 0, used_device=storage_device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    encodes all words in a given list to corresponding vectors in given model.\n",
    "    words not found in the model will be given a vector with \"default\" value\n",
    "    \n",
    "    Args:\n",
    "        sentence (list): list of strings (tokenized sentence)\n",
    "        model (Word2Vec): model to use when encoding\n",
    "        default (int): fill vector with this value if word is not found in model\n",
    "    \n",
    "    Returns:\n",
    "        np.array: 2d array with dim1 = len(sentence) and dim2 = model.vector_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate inital array with default values\n",
    "    vectorized = np.ones((len(segment), model.vector_size)) * default\n",
    "    \n",
    "    # loop over every word in list\n",
    "    for current_word, current_word_idx in zip(segment, range(len(segment))):\n",
    "        # only add correct values if word is in model, otherwise leave as default\n",
    "        if current_word in model.wv:\n",
    "            # the try except block is needed because (current_word in model.wv) sometimes gives a false positive... yeah gensim\n",
    "            try:\n",
    "                vectorized[current_word_idx] = model.wv.get_vector(current_word)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    vectorized = torch.tensor(vectorized, dtype=torch.float32, device=used_device)\n",
    "    \n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devectorize_segment(vectorized_segment: torch.Tensor, model: Word2Vec=embeddings_model, not_in_vocab_token=\"[NIV]\", NIV_threshold=0.01) -> list:\n",
    "    \"\"\"\n",
    "    decodes vectors into nearest word found in model, if no near words found, adds a not in vocab token\n",
    "    \n",
    "    Args:\n",
    "        vectorized_sentence (np.array): 2d arrat with vectors of words to be decoded\n",
    "        model (Word2Vec): model to use when decoding\n",
    "    \n",
    "    Returns:\n",
    "        list: list of strings (words) whos vectors most closely match those provided\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # make sure vectors are ready to be processed\n",
    "    vectorized_segment = vectorized_segment.cpu().numpy()\n",
    "    \n",
    "    # go over all words and find closest match in model\n",
    "    for current_word in vectorized_segment:\n",
    "        similarities = model.wv.similar_by_vector(current_word)\n",
    "        \n",
    "        # check if its not a bullshit vector\n",
    "        if similarities[0][1] > NIV_threshold:\n",
    "            result.append(similarities[0][0])\n",
    "        else:\n",
    "            result.append(not_in_vocab_token)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(suspected_tensor: torch.tensor, target_length: int, default: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pads or truncates a given tensor along dim 0 to target_length with \"default\" as padding\n",
    "    \n",
    "    Args:\n",
    "        suspected_tensor (torch.tensor): tensor to pad or truncate\n",
    "        target_length (int): target length of tensor\n",
    "        default (int): value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(suspected_tensor) < target_length:\n",
    "        # pad\n",
    "        suspected_tensor = torch.cat((torch.ones(target_length - len(suspected_tensor), suspected_tensor.shape[1], dtype=torch.float32, device=suspected_tensor.device) * default, suspected_tensor))\n",
    "    else:\n",
    "        # truncate\n",
    "        suspected_tensor = suspected_tensor[-target_length:]\n",
    "    \n",
    "    return suspected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segment_for_net(segment: list[str], length: int=context_length, used_device: torch.DeviceObjType=storage_device):\n",
    "    \"\"\"\n",
    "    function to take a sentence, and do everything to make it possible to input into the net\n",
    "    \n",
    "    Args:\n",
    "        segment (list[str]): a list of tokens (ideally from the tokenizer) of a sentence / text\n",
    "        length (int): the number of tokens to which pad or truncate to. for correct operation: keep at the net's context length\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: tokenized segment in the correct length\n",
    "    \"\"\"\n",
    "    \n",
    "    # turn into embedding vectors\n",
    "    vectorized = vectorize_segment(segment, used_device=used_device)\n",
    "    \n",
    "    # trim / add into length\n",
    "    trimmed = pad_or_truncate(vectorized, length)\n",
    "    \n",
    "    # add fake batch dimension\n",
    "    batched = trimmed.unsqueeze(0)\n",
    "    \n",
    "    return batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(segment: list[str], net: REAN=net):\n",
    "    # turn tokenized text into net's format\n",
    "    prepared_segment = prepare_segment_for_net(segment, used_device=next(net.parameters()).device)\n",
    "    \n",
    "    # run net\n",
    "    prediction_vector = net(prepared_segment).detach()\n",
    "    \n",
    "    # turn vector back into token\n",
    "    predicted_token = devectorize_segment(prediction_vector)\n",
    "    \n",
    "    return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(segment: list[str], num_tokens: int, net: REAN=net):\n",
    "    result = segment.copy()\n",
    "    \n",
    "    for _ in tqdm(range(num_tokens)):\n",
    "        result += predict_word(result, net=net)\n",
    "    \n",
    "    return result[len(segment):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   BUILD DATASET   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN_dataset(Dataset):\n",
    "    def pull_tokens(self, start_read_idx: int, requested_num_tokens: int):\n",
    "        \"\"\"\n",
    "        function returns a requested number of tokens from the dataset file, starting at APPROXIMATLY the start_read_idx token.\\n\n",
    "        attempts to return full words as much as possible, example:\\n\n",
    "        NO:    this | is | a | sen (tence)\\n\n",
    "        YES:   this | is | a | sentence\n",
    "        \n",
    "        Args:\n",
    "            start_read_idx (int): the APPROXIMATE token at which to start the reading (determined from the avarage token length in the tokenizer vocab)\n",
    "            requested_num_tokens (int): how many tokens to return\n",
    "        \n",
    "        Returns:\n",
    "            tokenized text (list of str): the tokens of the dataset from start_read_idx to start_read_idx + requested_num_tokens\n",
    "            is EOF hit (bool): if the requested args were outside of the dataset's range\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(self.path, errors=\"ignore\") as self.dataset:\n",
    "            self.dataset.seek(start_read_idx * tokenizer.average_token_length)\n",
    "            \n",
    "            # get an initial estimate to what text we will actually need\n",
    "            self.buffer = self.dataset.read(requested_num_tokens * tokenizer.average_token_length)\n",
    "            self.tokenized_buffer = tokenizer.tokenize_segment(self.buffer)\n",
    "            self.current_num_tokens = len(self.tokenized_buffer)\n",
    "            \n",
    "            # if the estimate we took is too small, we enlarge it character by character until its perfect\n",
    "            while self.current_num_tokens < requested_num_tokens + 1:\n",
    "                self.next_char = self.dataset.read(1)  # seperate variable to check EOF\n",
    "                \n",
    "                # check eof\n",
    "                if not self.next_char:\n",
    "                    print(\"pull_tokens(): eof was hit\")\n",
    "                    return self.tokenized_buffer[-requested_num_tokens - 1:][:-1], True\n",
    "                \n",
    "                self.buffer += self.next_char\n",
    "                \n",
    "                self.tokenized_buffer = tokenizer.tokenize_segment(self.buffer)\n",
    "                self.current_num_tokens = len(self.tokenized_buffer)\n",
    "        \n",
    "        # regardless of if the estimate is too long / short, return theproper amount of tokens, with the end snipped of, because it might be a half token\n",
    "        return self.tokenized_buffer[-requested_num_tokens - 1:][:-1], False\n",
    "    \n",
    "    def construct_example(self, start_read_idx: int):\n",
    "        \"\"\"\n",
    "        function to make a full datapoint, can be used as raw return for __getitem__()\n",
    "        \n",
    "        Args:\n",
    "            start_read_idx (int): at which token to start making the example\n",
    "        \n",
    "        Returns:\n",
    "            tokenized text (list of str): the tokens of the dataset from start_read_idx to start_read_idx + self.context_length\n",
    "        \"\"\"\n",
    "        \n",
    "        # pull neccesary amount of tokens for question / input and answer / output\n",
    "        self.tokens, _ = self.pull_tokens(start_read_idx, self.context_length + 1)\n",
    "        \n",
    "        # encode the tokens to vectors (aka embeddings)\n",
    "        self.vectorized_tokens = prepare_segment_for_net(self.tokens, length=self.context_length + 1).squeeze(0)\n",
    "        \n",
    "        # split into network input and expected output\n",
    "        self.question = self.vectorized_tokens[:-1] # everythinbg up to last word\n",
    "        self.answer = self.vectorized_tokens[-1] # last word itself\n",
    "        \n",
    "        return self.question, self.answer\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        function to read thru the whole dataset, and report how many examples there are / if there are as many as the user requested\n",
    "        \n",
    "        Args:\n",
    "            none, but uses self.num_tokens and self.context_length\n",
    "        \n",
    "        Returns:\n",
    "            returns how many usable examples there are, for __len__()\n",
    "        \"\"\"\n",
    "        \n",
    "        with tqdm(total=self.num_examples, desc=\"Calculating Dataset Size\", unit=\"example\") as pbar:\n",
    "            for self.current_check in range(self.num_examples):\n",
    "                _, self.eof = self.pull_tokens(self.current_check, self.context_length)\n",
    "                \n",
    "                if self.eof:\n",
    "                    print(\"The requested size is bigger than the .txt provided, so the dataset might be smaller than what you expected.\")\n",
    "                    break\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"Requested num_examples: {self.num_examples}\\nActual size found:      {self.current_check - 1}\")\n",
    "        \n",
    "        return self.current_check - 1   # the -1 is just in case\n",
    "    \n",
    "    def __init__(self, path, num_examples, context_length, embeddings_model, verify_dataset_size=True):\n",
    "        # transfer to object wide variables\n",
    "        self.path = path\n",
    "        self.context_length = context_length\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.num_examples = num_examples\n",
    "        \n",
    "        # get the size of the dataset txt file\n",
    "        self.dataset_len = num_examples\n",
    "        \n",
    "        if verify_dataset_size:\n",
    "            self.dataset_len = self.get_size()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.construct_example(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = REAN_dataset(train_dataset_path, examples_train, context_length, embeddings_model, verify_dataset_size=False)\n",
    "test_dataset = REAN_dataset(test_dataset_path, examples_test, context_length, embeddings_model, verify_dataset_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1073741824"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please validate dataset: does this look correct?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce crime and violence by keeping young people engaged in positive activities during vulnerable times.  2. The Ceasefire program: This program involves working closely with community members, law enforcement, and other organizations to target gun violence in inner city areas. The program emphasizes the value of community involvement and the importance of nonviolent conflict resolution.  3.    --->   The \n",
      "and violence by keeping young people engaged in positive activities during vulnerable times.  2. The Ceasefire program: This program involves working closely with community members, law enforcement, and other organizations to target gun violence in inner city areas. The program emphasizes the value of community involvement and the importance of nonviolent conflict resolution.  3. The Boys    --->   & \n"
     ]
    }
   ],
   "source": [
    "print(\"please validate dataset: does this look correct?\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    rnd_offset = random.randint(2097152 - 10000, 2097152 - 10000)\n",
    "    \n",
    "    for idx in range(2):\n",
    "        print(f\"{tokenizer.detokenize_segment(devectorize_segment(train_dataset[idx + rnd_offset][0].detach(), embeddings_model))}   --->   {tokenizer.detokenize_segment(devectorize_segment(train_dataset[idx + rnd_offset][1].detach().unsqueeze(0), embeddings_model))}\".replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if num_workers arg is used\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_graph = []\n",
    "test_loss_graph = []\n",
    "learning_rate_graph = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "with tqdm(total=train_epochs) as pbar:\n",
    "    batch = 0\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # training loop\n",
    "        for current_segment, target in train_loader:\n",
    "            batch += 1\n",
    "            \n",
    "            # move batch to gpu\n",
    "            current_segment = current_segment.to(run_device)\n",
    "            target = target.to(run_device)\n",
    "            \n",
    "            # train batch\n",
    "            train_outputs = net(current_segment)\n",
    "            train_loss_value = loss(train_outputs, target)\n",
    "            train_loss_value.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # collect performance metrics\n",
    "            train_loss_graph.append(train_loss_value.item())\n",
    "            \n",
    "            # plot everything\n",
    "            if batch % plot_batches == 0:\n",
    "                if plot_graphs:\n",
    "                    clear_output(wait=True)\n",
    "                    \n",
    "                    pbar.refresh()\n",
    "                    \n",
    "                    plt.figure(figsize=(15, 5), facecolor=background_color)\n",
    "\n",
    "                    # Plot training loss\n",
    "                    ax1 = plt.subplot(1, 3, 1, facecolor=background_color)\n",
    "                    plt.plot(train_loss_graph, label='Train Loss', color=train_loss_color)\n",
    "                    plt.xlabel('Epoch', color=text_color)\n",
    "                    plt.ylabel('Loss', color=text_color)\n",
    "                    plt.legend(facecolor=background_color, edgecolor='none', labelcolor=text_color)\n",
    "                    # Set spines to white\n",
    "                    ax1.spines['top'].set_color(axis_color)\n",
    "                    ax1.spines['bottom'].set_color(axis_color)\n",
    "                    ax1.spines['left'].set_color(axis_color)\n",
    "                    ax1.spines['right'].set_color(axis_color)\n",
    "                    # Set tick colors to white\n",
    "                    ax1.tick_params(axis='x', colors=axis_color)\n",
    "                    ax1.tick_params(axis='y', colors=axis_color)\n",
    "\n",
    "                    # Plot testing loss\n",
    "                    ax2 = plt.subplot(1, 3, 2, facecolor=background_color)\n",
    "                    plt.plot(range(0, len(test_loss_graph) * 3, 3), test_loss_graph, label='Test Loss', color=test_loss_color)\n",
    "                    plt.xlabel('Epoch', color=text_color)\n",
    "                    plt.ylabel('Loss', color=text_color)\n",
    "                    plt.legend(facecolor=background_color, edgecolor='none', labelcolor=text_color)\n",
    "                    # Set spines to white\n",
    "                    ax2.spines['top'].set_color(axis_color)\n",
    "                    ax2.spines['bottom'].set_color(axis_color)\n",
    "                    ax2.spines['left'].set_color(axis_color)\n",
    "                    ax2.spines['right'].set_color(axis_color)\n",
    "                    # Set tick colors to white\n",
    "                    ax2.tick_params(axis='x', colors=axis_color)\n",
    "                    ax2.tick_params(axis='y', colors=axis_color)\n",
    "\n",
    "                    # Plot learning rate\n",
    "                    ax3 = plt.subplot(1, 3, 3, facecolor=background_color)\n",
    "                    plt.plot(learning_rate_graph, label='Learning Rate', color=lr_color)\n",
    "                    plt.xlabel('Epoch', color=text_color)\n",
    "                    plt.ylabel('Learning Rate', color=text_color)\n",
    "                    plt.legend(facecolor=background_color, edgecolor='none', labelcolor=text_color)\n",
    "                    # Set spines to white\n",
    "                    ax3.spines['top'].set_color(axis_color)\n",
    "                    ax3.spines['bottom'].set_color(axis_color)\n",
    "                    ax3.spines['left'].set_color(axis_color)\n",
    "                    ax3.spines['right'].set_color(axis_color)\n",
    "                    # Set tick colors to white\n",
    "                    ax3.tick_params(axis='x', colors=axis_color)\n",
    "                    ax3.tick_params(axis='y', colors=axis_color)\n",
    "\n",
    "                    # Adjust layout and show plot\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "        # eval loop\n",
    "        if epoch % test_loop_epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                for test_current_segment, test_target in test_loader:\n",
    "                    # move batch to gpu\n",
    "                    test_current_segment = test_current_segment.to(run_device)\n",
    "                    test_target = test_target.to(run_device)\n",
    "                    \n",
    "                    # run test\n",
    "                    test_outputs = net(test_current_segment)\n",
    "                    test_loss_value = loss(test_outputs, test_target)\n",
    "                    \n",
    "                    # collect performance metrics\n",
    "                    test_loss_graph.append(test_loss_value.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # collect perforamce metrics\n",
    "        learning_rate_graph.append(optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:04<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is a great way to practice mindfulness or mindfulness exercises. Here are some tips for staying focused:\n",
      "\n",
      "1. Practice mindfulness - Practice meditation can help you stay focused and focused. This can help you stay focused and focused. 2. Practice mindfulness - Practice mindfulness can help you stay focused and focused. This can help you stay focused \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.detokenize_segment(predict_sequence(tokenizer.tokenize_segment(\"human: what are some good exercice routines? network: mindfullness meditation \"), 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, './REAN_nets/meth_abuser6969_attn_stack.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: out_head.fc1.weight\n",
      "Parameter value: tensor([[ 2.8501e-04,  1.7346e-03,  1.5170e-03,  ...,  8.6714e-03,\n",
      "         -7.6625e-03,  6.1953e-03],\n",
      "        [ 1.2689e-03, -6.0716e-04,  8.9317e-04,  ..., -1.3318e-03,\n",
      "          6.2218e-03, -3.5438e-02],\n",
      "        [-1.8463e-04, -3.2000e-03, -6.5099e-04,  ..., -2.5310e-03,\n",
      "          9.2636e-04, -1.6233e-03],\n",
      "        ...,\n",
      "        [-4.4252e-03,  9.5197e-04,  1.1865e-03,  ..., -2.5807e-03,\n",
      "          5.1965e-03, -1.7198e-02],\n",
      "        [ 3.9547e-04,  5.9387e-03,  2.2447e-03,  ...,  3.6341e-03,\n",
      "         -6.7722e-03, -5.2703e-03],\n",
      "        [ 6.5021e-05,  5.1800e-03,  8.5941e-05,  ...,  3.8153e-03,\n",
      "          4.7976e-04, -8.3834e-03]], device='cuda:0')\n",
      "Parameter name: out_head.fc1.bias\n",
      "Parameter value: tensor([-5.9697e-03, -1.3967e-02,  7.5924e-04, -3.2832e-03, -7.3408e-03,\n",
      "        -1.5117e-02, -1.6672e-02, -7.3852e-03, -6.2382e-03, -9.9220e-03,\n",
      "        -9.8753e-03, -1.9472e-03, -9.2611e-03, -5.7023e-03, -1.2467e-02,\n",
      "        -4.7410e-03, -1.8698e-03,  5.8805e-04, -4.7145e-03, -1.4201e-02,\n",
      "        -4.5717e-03, -1.1038e-02, -7.0159e-03, -6.8091e-03, -4.2292e-03,\n",
      "        -2.7744e-04, -9.0714e-03, -3.9701e-03,  6.6930e-04, -9.8784e-03,\n",
      "        -1.0902e-02, -4.1106e-03, -6.0665e-03, -1.0030e-02, -6.0645e-03,\n",
      "        -6.9439e-03, -3.6260e-03, -6.3621e-03, -7.3633e-03, -6.1456e-03,\n",
      "        -9.8941e-03, -1.2637e-02, -9.0951e-03, -3.1812e-03, -8.1380e-03,\n",
      "        -3.5355e-03, -1.4063e-02, -1.0976e-02, -3.3313e-03, -2.7155e-03,\n",
      "        -1.4882e-02, -8.0240e-04, -9.6114e-03, -1.9370e-03, -2.8736e-03,\n",
      "        -7.3622e-03, -7.8151e-03, -4.5633e-03, -6.5805e-03, -1.0000e-02,\n",
      "        -1.1835e-02, -5.2610e-03,  1.1550e-03, -7.1143e-03, -9.1503e-03,\n",
      "        -1.3441e-02,  4.3824e-03, -1.3012e-02, -7.4215e-03, -4.2307e-03,\n",
      "        -5.5955e-03, -3.1275e-04, -6.7368e-03, -9.0589e-03, -1.2894e-02,\n",
      "        -6.0910e-03, -2.5080e-03, -1.8196e-02, -8.1165e-03,  1.9867e-03,\n",
      "        -3.2475e-03, -1.3124e-02, -9.7632e-03, -1.2902e-02, -3.1274e-03,\n",
      "        -1.0898e-02, -1.2030e-02, -2.2713e-03, -9.9093e-03, -1.2317e-02,\n",
      "         3.6356e-03, -1.6074e-02, -7.1295e-03, -9.2665e-03, -6.4692e-03,\n",
      "        -6.0996e-03, -7.3591e-03, -1.4717e-02,  9.3114e-04, -1.7644e-02,\n",
      "        -1.2943e-03, -1.6428e-03, -3.1562e-03, -2.0583e-02, -8.6067e-03,\n",
      "        -1.7733e-03, -8.4172e-03, -2.6088e-03,  2.3083e-04, -5.6120e-03,\n",
      "        -1.0038e-03, -7.4171e-03, -3.5910e-03,  1.5923e-03, -1.0282e-02,\n",
      "        -5.6408e-03, -1.3614e-02, -8.3115e-03, -8.0333e-03, -1.1820e-03,\n",
      "        -1.0979e-02, -4.5521e-03, -4.4880e-03, -1.6904e-02, -7.3833e-03,\n",
      "        -1.4473e-02, -5.5622e-03, -1.5524e-02, -5.5168e-03, -6.7080e-03,\n",
      "        -6.4591e-03,  6.8339e-04, -1.0437e-02, -3.5499e-03, -9.9106e-03,\n",
      "        -7.0516e-03, -6.9850e-03, -7.6615e-03, -6.6830e-03,  1.0669e-03,\n",
      "         1.4961e-03, -8.0984e-03, -6.6544e-03, -6.0419e-03, -1.9660e-03,\n",
      "        -4.8038e-03, -1.3347e-02, -6.7104e-03, -7.9904e-03, -7.9286e-03,\n",
      "        -1.1697e-02, -9.6011e-03, -3.2061e-03, -4.5164e-03, -3.7677e-03,\n",
      "        -1.6004e-03, -8.2941e-03, -7.0795e-03, -6.8284e-03, -1.4126e-02,\n",
      "        -1.1714e-02, -5.3613e-03, -2.8038e-03, -4.0994e-04, -4.5572e-03,\n",
      "        -8.8420e-03,  9.0873e-04, -1.4890e-02, -1.5558e-02, -4.5571e-03,\n",
      "        -1.0238e-02, -1.9131e-03, -2.8352e-03, -1.4494e-02, -6.8061e-03,\n",
      "         6.5522e-04, -6.4699e-03, -1.0469e-02, -1.5211e-02, -8.2654e-03,\n",
      "        -6.0771e-03, -6.8323e-03, -3.8488e-04, -5.9183e-03, -1.2319e-02,\n",
      "        -6.2795e-03, -8.1861e-03, -1.4658e-02, -4.7889e-03, -6.2441e-03,\n",
      "        -8.5903e-04, -2.0791e-03, -1.4049e-02, -1.5357e-02, -8.3378e-03,\n",
      "        -1.4233e-02, -1.0354e-02, -5.6864e-03, -5.2278e-03, -6.9158e-03,\n",
      "        -1.1026e-02, -5.9568e-03, -5.1763e-03, -6.4046e-03, -2.6660e-03,\n",
      "        -4.2659e-03, -5.1014e-03, -1.1302e-02, -1.3671e-02,  5.5928e-04,\n",
      "        -3.3596e-03, -1.3796e-02, -1.2106e-02, -5.9822e-03, -6.5288e-03,\n",
      "        -8.7128e-03, -8.3841e-03, -5.6243e-03, -3.4387e-03, -8.3431e-04,\n",
      "        -1.5856e-03, -5.8053e-03, -1.4902e-02, -1.0636e-02, -7.8882e-03,\n",
      "        -1.3562e-02, -3.5004e-03, -4.3722e-03, -7.3988e-03, -5.7031e-03,\n",
      "        -4.8717e-03, -1.2321e-02, -1.0266e-02,  1.3430e-03, -6.8850e-03,\n",
      "        -7.0039e-03, -1.1333e-02, -1.0098e-02, -9.5664e-04, -7.3032e-03,\n",
      "        -1.4791e-02, -7.4103e-03, -1.2535e-02, -8.5613e-03, -5.1180e-03,\n",
      "         2.0466e-03, -3.7456e-03, -7.7906e-04, -2.3882e-03, -1.0097e-02,\n",
      "        -1.0449e-02, -1.1724e-02, -1.0789e-02, -4.1888e-03, -8.5026e-03,\n",
      "        -7.7692e-03, -9.8460e-03, -9.9327e-03,  5.7032e-04, -8.3432e-03,\n",
      "        -4.7909e-03, -2.8671e-03, -1.4069e-02, -9.4127e-03, -2.2805e-03,\n",
      "        -7.4958e-03, -3.4784e-03, -1.2285e-02, -4.6793e-03,  7.6229e-04,\n",
      "        -9.9717e-03, -2.0277e-03, -5.0223e-03, -5.8175e-04, -5.0040e-03,\n",
      "        -7.9444e-03, -9.7451e-03, -1.2240e-03, -8.8206e-03, -5.4620e-03,\n",
      "        -1.1356e-02, -9.1536e-03, -1.3761e-02, -5.8564e-03, -9.5411e-03,\n",
      "        -9.4576e-03, -1.1198e-02, -1.0181e-02, -5.2408e-03, -4.2746e-03,\n",
      "        -9.5617e-03, -3.9623e-03, -1.5626e-02, -1.5428e-02, -1.0168e-02,\n",
      "        -1.2063e-03, -6.3755e-03, -3.5181e-03, -1.6900e-02, -1.6278e-02,\n",
      "        -5.0139e-03, -9.8227e-03, -1.0623e-02, -1.1982e-02, -9.0659e-03,\n",
      "        -4.0349e-03, -8.9526e-03, -1.4849e-02, -3.8101e-03, -9.1800e-03,\n",
      "        -1.1067e-02, -1.6619e-02,  1.6936e-03, -4.6420e-03, -7.5687e-03,\n",
      "        -4.7214e-03, -5.9867e-03, -3.2837e-03, -7.2510e-03, -1.2239e-02,\n",
      "        -6.7789e-03, -7.2943e-03, -1.5158e-02,  8.8986e-03, -2.5101e-03,\n",
      "        -1.4276e-03, -1.3599e-02, -4.0290e-04, -1.5824e-02, -6.1182e-03,\n",
      "        -6.6694e-03, -6.1755e-03, -3.5965e-03, -3.1656e-03, -8.7171e-03,\n",
      "        -8.3450e-03, -5.6513e-03, -1.0025e-02,  6.1710e-03, -8.4954e-03,\n",
      "        -1.2130e-02, -7.0421e-03, -8.6524e-03, -6.7329e-03, -1.2142e-02,\n",
      "        -1.8574e-02, -1.3242e-02, -4.4120e-03, -5.3328e-03, -6.8232e-03,\n",
      "        -5.4870e-03, -1.9326e-02, -3.8946e-03, -1.2100e-02, -3.2806e-03,\n",
      "         4.2049e-04, -4.6459e-03, -7.4811e-03, -2.2710e-03, -1.3512e-02,\n",
      "        -1.3686e-02, -6.9248e-03, -4.3322e-03, -7.2998e-03, -1.8091e-03,\n",
      "        -1.4913e-02, -1.1083e-02, -1.2078e-02, -1.3964e-02, -1.1177e-02,\n",
      "        -1.0526e-02, -8.1885e-04, -4.6577e-03, -9.2720e-03, -6.2459e-03,\n",
      "        -6.6047e-03, -7.6581e-03, -3.8685e-03, -1.1481e-04, -1.4066e-02,\n",
      "        -1.4102e-02, -6.1009e-03, -1.3342e-02, -3.7779e-03, -1.3388e-02,\n",
      "        -6.5290e-03, -2.8752e-03, -1.6757e-03, -9.8395e-03, -2.2439e-03,\n",
      "         6.1932e-04, -8.9541e-03, -1.0283e-02, -2.9495e-03, -3.7702e-04,\n",
      "        -1.0280e-02, -1.0106e-02, -3.8866e-04, -1.3975e-02, -9.1157e-03,\n",
      "        -1.5311e-02, -1.2983e-02, -1.4846e-02, -1.2869e-02, -2.6969e-03,\n",
      "         1.5289e-03, -6.6838e-03, -1.1651e-02, -1.5324e-02, -5.2144e-03,\n",
      "         1.8086e-03, -1.2415e-02, -3.8504e-03, -1.0541e-02, -3.8070e-03,\n",
      "        -5.6359e-03, -1.6192e-02, -3.9322e-03, -1.2378e-02, -6.4414e-03,\n",
      "        -1.3233e-02, -3.4560e-03, -3.2677e-04, -1.1262e-02, -1.3074e-02,\n",
      "        -1.0304e-03, -5.4420e-03, -2.7566e-03, -2.4548e-03, -3.9575e-03,\n",
      "        -5.9767e-03, -4.6120e-03,  4.1354e-04, -3.3213e-03, -6.6961e-03,\n",
      "        -1.5186e-02, -6.8894e-03, -1.2721e-02, -9.0026e-03, -1.4571e-02,\n",
      "        -8.9623e-03, -3.4215e-03,  1.9977e-03, -1.0545e-02, -1.3626e-02,\n",
      "        -1.4896e-02, -8.5799e-03, -1.3448e-02, -1.3209e-02, -9.5599e-03,\n",
      "        -3.5290e-03, -8.0551e-03, -1.2396e-02, -2.2759e-03, -1.1035e-02,\n",
      "         2.4253e-03, -9.4507e-03, -6.5117e-03, -3.2769e-03, -1.3801e-02,\n",
      "        -2.1451e-02, -1.8231e-02, -1.4083e-02, -1.0522e-02, -9.6199e-03,\n",
      "        -9.9237e-03, -2.9441e-03, -6.2572e-03, -5.8061e-03, -8.8453e-03,\n",
      "         7.9457e-03, -8.9185e-03, -1.0735e-02, -1.2928e-02, -9.9564e-03,\n",
      "         2.3624e-03, -1.1660e-02, -1.8025e-02, -1.4181e-02, -8.2869e-03,\n",
      "        -6.1683e-03, -5.4747e-03, -4.1603e-03, -8.3573e-03, -6.8138e-03,\n",
      "        -8.8985e-03, -4.0682e-03, -1.9246e-03, -1.3622e-02, -9.1913e-04,\n",
      "        -1.0952e-02, -1.2240e-02, -1.6216e-03, -1.6861e-02, -6.2810e-03,\n",
      "        -5.6677e-03, -1.8855e-03, -9.3091e-03, -1.1639e-02, -9.1883e-03,\n",
      "        -1.0990e-02, -5.1805e-03, -1.3298e-02, -7.0744e-03, -1.0455e-02,\n",
      "        -1.3074e-02, -1.2256e-02, -7.7696e-03, -1.3698e-03, -7.6681e-03,\n",
      "        -1.0539e-02, -8.7175e-03, -1.7346e-02, -6.1927e-03, -9.6099e-03,\n",
      "        -6.8158e-03, -1.0866e-02, -8.6835e-03, -7.4853e-03, -1.2900e-02,\n",
      "        -1.0510e-02, -4.7216e-03, -1.5026e-03, -5.6937e-03, -1.6136e-03,\n",
      "        -1.5138e-04, -1.0599e-02, -6.6850e-03, -1.3204e-02, -4.2032e-03,\n",
      "        -1.5713e-03, -1.4532e-02, -2.8558e-03, -8.2334e-03, -1.1725e-02,\n",
      "        -1.1519e-02, -8.9999e-05, -3.5249e-03, -1.1751e-02, -1.0151e-02,\n",
      "        -8.3871e-03, -1.6076e-02, -3.2879e-03, -1.1177e-02, -1.1448e-02,\n",
      "        -6.9573e-03,  2.1447e-03, -2.7135e-03, -1.4207e-02, -5.9262e-03,\n",
      "        -9.4475e-03, -9.1822e-03, -3.7796e-03, -1.1651e-02, -1.0806e-02,\n",
      "        -1.6700e-02, -8.7545e-03,  1.9787e-04, -1.0841e-02, -3.6679e-04,\n",
      "        -1.2689e-02, -1.1973e-02, -1.0117e-03, -9.3370e-03, -2.9731e-03,\n",
      "        -4.8151e-03, -1.9500e-02, -6.7869e-03, -1.6093e-03,  1.0361e-03,\n",
      "        -1.3930e-02, -3.9651e-03, -6.9684e-03, -1.4655e-02, -1.3012e-02,\n",
      "        -1.1163e-03, -2.2333e-03, -7.9612e-04,  8.2953e-04, -5.4146e-03,\n",
      "        -3.2651e-03, -1.2920e-02, -8.4449e-03, -1.0162e-02, -1.4865e-02,\n",
      "        -1.0980e-02, -1.2705e-02, -6.3848e-03, -1.0796e-02, -5.5538e-03,\n",
      "        -2.2462e-03, -5.7859e-03, -8.6878e-03, -7.4674e-03, -6.7179e-03,\n",
      "        -4.5297e-03, -5.0209e-03, -1.1219e-02, -8.0338e-03, -5.9832e-03,\n",
      "        -7.0527e-03, -1.1128e-02, -9.5207e-03, -1.7256e-02, -2.0923e-03,\n",
      "        -5.2532e-03, -1.2253e-02, -1.4167e-02, -2.8575e-03,  7.9361e-04,\n",
      "        -8.8211e-03, -7.1340e-03, -4.0771e-03, -6.7283e-03, -1.1915e-02,\n",
      "        -7.0520e-03,  7.7203e-04, -9.2677e-03, -4.8746e-03, -9.9499e-03,\n",
      "        -1.4695e-02, -9.0104e-03, -1.0073e-02, -1.3117e-02, -2.6712e-03,\n",
      "        -8.0005e-03, -4.3743e-03, -8.8050e-03, -5.9829e-03,  1.6330e-03,\n",
      "        -5.6357e-03, -7.4977e-03, -9.4369e-03, -3.1688e-03, -4.8089e-03,\n",
      "        -1.3202e-02, -1.4609e-02, -8.4655e-03, -1.5284e-02, -5.5700e-03,\n",
      "        -1.9201e-03, -5.4780e-03, -1.2268e-02, -1.0539e-02, -8.1712e-03,\n",
      "        -6.3205e-03, -1.1329e-02, -1.2108e-02, -2.7212e-03, -1.1399e-02,\n",
      "         2.0810e-03, -1.1843e-02, -1.9846e-02, -8.3069e-03, -1.4451e-02,\n",
      "        -9.6591e-03, -3.9452e-03, -1.0199e-02, -1.7018e-02, -1.2014e-02,\n",
      "        -1.9128e-03, -9.8208e-03, -8.1970e-03, -4.7847e-03, -5.9773e-03,\n",
      "        -5.2350e-03, -1.5851e-02, -9.3743e-03, -1.1940e-02, -9.0478e-03,\n",
      "        -3.4609e-03, -2.0521e-02,  2.9864e-03, -1.7972e-03, -1.3119e-03,\n",
      "        -7.4186e-03,  1.4618e-03, -8.8456e-03, -3.7053e-03, -1.4563e-02,\n",
      "        -4.9485e-03, -3.5790e-03, -7.0967e-03, -8.9409e-03, -1.3430e-02,\n",
      "        -1.1017e-02,  7.9302e-04, -1.1154e-02, -1.1822e-02, -5.1275e-03,\n",
      "        -1.1894e-02, -1.2421e-02, -1.5821e-04, -7.6438e-03, -3.3731e-03,\n",
      "        -5.2624e-03, -5.2957e-03, -4.3047e-03, -1.5864e-02, -1.2686e-02,\n",
      "        -9.5295e-03, -3.4822e-03, -1.2054e-02, -7.2628e-03, -1.1635e-02,\n",
      "        -8.5919e-03, -1.6930e-04, -9.2616e-03, -1.3696e-02, -3.6015e-03,\n",
      "        -8.3547e-03, -7.8661e-03, -7.5246e-03, -7.2322e-03, -1.3229e-02,\n",
      "         1.7801e-03, -1.7703e-02, -1.7969e-02, -1.3208e-02, -9.9952e-03,\n",
      "        -1.0993e-02, -2.9657e-03, -9.7310e-04, -1.0291e-02, -6.9982e-03,\n",
      "        -4.5290e-03, -3.3342e-03, -9.0303e-03, -5.6354e-03, -1.1597e-02,\n",
      "        -1.1883e-02, -1.5675e-02,  1.1559e-03, -1.1840e-02, -7.8182e-03,\n",
      "        -9.9441e-03, -4.2700e-03, -8.7044e-03, -4.6137e-03, -4.8239e-03,\n",
      "        -4.2836e-03, -8.5084e-03, -7.2390e-03, -3.1773e-03, -9.1191e-03,\n",
      "        -8.3498e-03, -7.3296e-03, -3.2771e-03, -4.1615e-03, -1.2664e-02,\n",
      "        -1.1683e-02, -3.6587e-03, -1.2191e-02, -7.4391e-03, -1.3358e-02,\n",
      "        -9.1530e-03, -1.8264e-02, -1.4560e-02, -4.4185e-03, -1.4970e-02,\n",
      "        -1.1557e-02,  4.9519e-04, -1.0437e-02, -6.0006e-03, -8.7565e-03,\n",
      "        -9.0215e-03, -4.9106e-03, -3.7799e-03, -8.6819e-05, -1.3308e-03,\n",
      "        -1.1670e-02, -1.6432e-03, -6.5103e-03, -1.6704e-02, -8.9223e-03,\n",
      "        -4.3202e-03, -9.6176e-03, -1.0725e-02,  6.8012e-04, -8.7942e-03,\n",
      "        -1.1788e-02, -1.3507e-02, -5.9347e-03, -7.5199e-03], device='cuda:0')\n",
      "Parameter name: out_head.fc2.weight\n",
      "Parameter value: tensor([[ 0.0007, -0.0182, -0.0223,  ..., -0.0057,  0.0478,  0.0478],\n",
      "        [ 0.0091, -0.0007,  0.0143,  ..., -0.0287, -0.0274,  0.0257],\n",
      "        [-0.0055, -0.0116, -0.0218,  ..., -0.0417,  0.0079, -0.0163],\n",
      "        ...,\n",
      "        [ 0.0197, -0.0055, -0.0003,  ...,  0.0099, -0.0056, -0.0471],\n",
      "        [ 0.0252,  0.0009, -0.0158,  ...,  0.0192,  0.0154,  0.0137],\n",
      "        [ 0.0328,  0.0296, -0.0216,  ...,  0.0008,  0.0041,  0.0048]],\n",
      "       device='cuda:0')\n",
      "Parameter name: out_head.fc2.bias\n",
      "Parameter value: tensor([-3.5978e-02, -1.4305e-02,  1.0970e-02, -3.2204e-02,  3.1336e-02,\n",
      "         2.9794e-02,  1.3352e-02,  2.1311e-02,  2.1005e-02,  3.7497e-02,\n",
      "         1.3456e-02, -4.7278e-03, -2.9645e-02, -4.3675e-02, -1.0864e-02,\n",
      "         2.2019e-02, -1.5974e-02, -3.2140e-03, -7.4106e-03,  5.2461e-03,\n",
      "        -2.3494e-02,  2.9568e-02, -1.6555e-02, -1.7652e-02, -2.3014e-02,\n",
      "        -3.8279e-02,  3.8190e-02, -7.5625e-03, -1.5589e-02, -1.5754e-02,\n",
      "         3.3694e-03, -5.1195e-02,  4.3303e-03, -2.4768e-02, -3.2108e-02,\n",
      "         1.9122e-02, -1.3159e-02,  6.2191e-03,  1.1052e-03, -1.2871e-02,\n",
      "        -1.2441e-02,  1.6464e-02, -3.9644e-02,  2.5329e-02,  1.4948e-02,\n",
      "         4.7595e-02,  6.8423e-03,  2.8859e-02,  2.1750e-02,  4.5934e-02,\n",
      "         1.4926e-02,  3.0702e-02, -9.8438e-03,  1.2529e-02,  1.5375e-02,\n",
      "        -1.6830e-02, -2.2010e-02,  2.8083e-02, -5.6366e-03, -1.9302e-02,\n",
      "        -3.0221e-02, -2.6199e-02,  5.1819e-03,  2.9701e-02, -1.2858e-02,\n",
      "        -6.7859e-04, -6.4071e-03, -5.7017e-03, -1.0272e-02, -4.7650e-03,\n",
      "        -3.7542e-02,  7.5985e-03,  2.7282e-03,  1.0369e-02,  2.4782e-03,\n",
      "         1.8871e-02, -1.9274e-02,  3.2286e-02,  3.3690e-02,  5.8921e-03,\n",
      "         1.3732e-02,  1.1166e-02, -2.5632e-02,  9.1658e-03,  1.9121e-02,\n",
      "         1.2761e-02,  2.7541e-02,  2.5851e-02,  3.5356e-02,  2.6785e-02,\n",
      "        -9.8644e-03,  2.0396e-02,  2.6157e-02, -2.6969e-04,  2.9913e-02,\n",
      "        -3.3193e-02,  3.5976e-02, -1.4862e-02, -2.4367e-02,  1.6150e-02,\n",
      "        -6.5568e-03,  2.5717e-02, -1.8880e-02, -2.4899e-02, -1.7405e-02,\n",
      "         1.0996e-02, -2.2899e-02, -1.9126e-02,  7.6196e-03,  7.0981e-03,\n",
      "        -2.1273e-03, -1.0928e-03,  8.0774e-03,  3.8170e-02, -7.7822e-03,\n",
      "         6.8389e-03, -3.6881e-02, -2.6179e-02,  4.3410e-02,  2.1735e-02,\n",
      "         1.9253e-02, -1.5064e-02,  1.1471e-02,  2.3819e-02,  2.2114e-03,\n",
      "        -1.2068e-02,  7.7215e-03,  1.6414e-02,  2.9167e-02, -2.3542e-02,\n",
      "         1.0612e-02, -2.5462e-02, -1.1563e-02, -1.6153e-02, -1.5869e-02,\n",
      "        -7.3179e-03,  3.5134e-02, -1.3534e-02, -2.3618e-02,  9.9646e-03,\n",
      "        -2.7861e-02, -1.5710e-02,  1.0927e-02,  2.5553e-02,  4.1422e-02,\n",
      "        -1.0760e-02, -3.4047e-02, -2.8208e-02, -8.1804e-03, -1.1452e-02,\n",
      "         1.2125e-02, -4.6006e-03,  2.7522e-03,  1.6552e-02, -8.4468e-03,\n",
      "        -3.4078e-03, -2.4323e-02, -2.7774e-02,  5.6755e-03,  7.7304e-04,\n",
      "         2.4302e-02,  1.8911e-02, -2.1699e-03,  1.1309e-02,  3.8405e-02,\n",
      "         3.0005e-02, -2.1592e-02, -2.8921e-02,  5.0171e-03,  6.7765e-03,\n",
      "        -2.6671e-02,  3.6911e-02, -3.0584e-02, -1.8746e-04,  2.3674e-02,\n",
      "         2.0823e-02,  5.5118e-03,  3.5422e-02,  5.7825e-03,  1.4876e-02,\n",
      "        -1.0900e-02,  3.8731e-02, -3.8014e-02, -3.6714e-02, -1.6048e-02,\n",
      "         1.5052e-02,  2.2524e-02, -3.2251e-02,  2.1257e-02,  1.1782e-03,\n",
      "        -1.9168e-02, -2.3731e-02,  3.4747e-02,  2.3843e-03,  7.2599e-03,\n",
      "         1.1438e-03,  1.3559e-02, -4.3226e-02, -1.0024e-02, -4.7690e-02,\n",
      "         6.7739e-04, -3.5670e-02, -1.2469e-02, -3.3668e-02,  2.4790e-02,\n",
      "         6.1005e-03,  2.1497e-02,  4.9372e-03, -2.8527e-02, -2.4206e-02,\n",
      "        -2.8461e-02,  8.9343e-03, -4.6165e-03,  1.7548e-02,  2.8452e-02,\n",
      "        -4.4673e-02,  2.0818e-02, -2.0740e-02,  1.7261e-03,  6.6345e-03,\n",
      "         1.5232e-02,  2.7988e-02,  3.2664e-02,  3.2844e-02,  9.5685e-04,\n",
      "         3.7293e-03,  2.2036e-02,  1.2087e-02, -3.0867e-02, -4.6153e-02,\n",
      "        -2.4938e-02, -3.4823e-02, -2.2605e-02,  2.4203e-02,  2.4461e-02,\n",
      "        -2.5181e-03,  4.9255e-02,  1.5584e-02,  5.6602e-03, -1.6362e-02,\n",
      "         7.8579e-04, -2.7842e-02, -1.0729e-02,  2.1260e-02,  3.7608e-03,\n",
      "         4.1423e-02,  3.0617e-02,  2.3161e-03,  3.3488e-02,  1.9886e-02,\n",
      "         1.6894e-02, -4.6027e-02,  2.7501e-02,  2.6622e-02,  2.4048e-02,\n",
      "         3.9121e-02, -2.5195e-02, -2.3187e-02, -2.6234e-02,  3.8688e-03,\n",
      "        -7.8364e-03, -6.7518e-03, -6.6059e-03,  7.1815e-03, -5.9273e-03,\n",
      "         1.5248e-02,  2.0916e-03,  1.3225e-02, -4.7443e-02, -3.3072e-02,\n",
      "        -5.5120e-04,  2.3328e-02, -2.1503e-02, -3.7526e-02, -3.7243e-04,\n",
      "         4.7283e-03, -1.6526e-02,  2.0677e-02,  2.8945e-02,  3.1025e-02,\n",
      "         1.9921e-02,  2.7362e-02, -4.6744e-02, -2.1509e-02, -2.6543e-03,\n",
      "        -4.5440e-03,  1.2922e-04, -7.4972e-03,  2.8966e-02, -4.7547e-03,\n",
      "         2.4356e-02,  8.9717e-03, -3.8367e-02, -1.3665e-02,  8.6597e-04,\n",
      "         3.1656e-02,  6.0831e-03,  2.5779e-02,  1.5278e-02, -2.8671e-02,\n",
      "         2.1933e-02, -1.5191e-02, -3.1745e-02, -9.8502e-04,  6.8924e-03,\n",
      "         8.6562e-03,  9.2824e-04, -3.5972e-02, -3.1278e-02,  4.8101e-02,\n",
      "         1.2197e-02,  1.7309e-02,  3.1815e-02, -2.2764e-02, -9.9320e-03,\n",
      "         6.6830e-03,  2.2947e-02, -2.4827e-02,  1.1476e-02, -2.5087e-02,\n",
      "         1.4290e-02, -1.9039e-02,  1.5755e-02,  6.6923e-04,  2.1739e-02,\n",
      "        -1.8897e-02,  2.9090e-02, -2.0660e-02,  8.1694e-03, -4.0028e-02,\n",
      "        -1.2563e-02,  4.4218e-02,  1.5780e-02, -4.6955e-02, -1.8401e-02,\n",
      "         1.3818e-02,  3.9722e-02,  2.6269e-02, -1.2136e-03, -1.0688e-02,\n",
      "         3.9472e-02,  3.7588e-05,  7.8059e-04, -4.0012e-03, -3.6781e-03,\n",
      "        -1.7409e-02,  1.4786e-02, -4.7598e-03, -7.1863e-03, -2.3435e-02,\n",
      "         1.1054e-02,  4.1552e-02,  2.8974e-02,  8.1039e-03,  1.2951e-02,\n",
      "        -2.0934e-02,  9.1157e-03, -2.2743e-02, -2.0044e-02, -9.6703e-03,\n",
      "        -4.7918e-02, -2.9502e-02, -2.0303e-02,  1.2924e-02,  6.2917e-03,\n",
      "        -2.9670e-02, -2.5580e-02, -2.1909e-03,  2.3750e-02,  1.9701e-02,\n",
      "        -9.6133e-03, -2.2731e-03, -5.1182e-02, -3.2752e-02,  2.9427e-02,\n",
      "        -1.9682e-03, -4.5938e-02, -2.9462e-02,  3.3227e-02, -5.3529e-03,\n",
      "         1.3053e-02, -2.4144e-02, -4.1174e-02, -2.0432e-02, -1.5760e-02,\n",
      "         8.7551e-03,  1.7228e-02, -1.7518e-02, -5.2177e-03,  1.7156e-02,\n",
      "        -7.7842e-03,  1.9691e-02,  9.1003e-03,  2.0405e-02,  1.6898e-02,\n",
      "         1.1017e-02, -1.9016e-02, -2.2451e-02, -2.2495e-02, -2.5837e-02,\n",
      "        -2.5283e-03, -7.5955e-03,  4.7773e-02, -1.1731e-02, -9.1838e-03,\n",
      "         4.0066e-03,  3.1303e-02,  4.8889e-03, -4.6068e-02,  1.4651e-02,\n",
      "         1.4643e-02, -3.5011e-02,  2.1229e-02, -2.4945e-02, -2.6533e-02,\n",
      "         6.5462e-03,  3.3567e-02, -2.4114e-03, -3.7586e-02,  1.4932e-02,\n",
      "         9.4797e-03, -3.9755e-04,  5.0135e-02, -4.4600e-02,  1.2637e-02,\n",
      "         3.2388e-02,  2.2770e-03, -5.2769e-04,  3.7129e-02,  3.3719e-02,\n",
      "        -8.0009e-03, -3.4071e-03,  1.1125e-02, -4.5171e-02,  2.1564e-02,\n",
      "        -1.4120e-02, -1.0382e-02, -4.8358e-02, -2.7655e-02, -1.3278e-02,\n",
      "         2.8201e-02, -1.1801e-02, -1.2133e-02,  2.2260e-02,  9.7341e-03,\n",
      "         1.4934e-02, -2.5419e-02, -1.7026e-02, -3.8962e-03, -3.7116e-03,\n",
      "        -1.0133e-02,  2.9734e-02, -1.8335e-02,  2.5497e-02, -2.6059e-02,\n",
      "         8.6126e-03, -1.6399e-03,  7.5695e-03,  3.2963e-02,  2.3612e-02,\n",
      "         1.7554e-02,  2.3727e-02, -1.1447e-02, -1.4968e-02,  1.2701e-02,\n",
      "         2.1338e-02,  2.3361e-02, -1.8479e-02,  1.3069e-02,  1.1636e-02,\n",
      "        -1.3496e-02, -2.6519e-02, -1.5755e-02, -1.6073e-03,  3.9019e-02,\n",
      "        -2.5613e-02, -3.8046e-02, -1.7882e-03,  2.2053e-03,  8.8854e-03,\n",
      "         6.0454e-03, -1.3427e-02,  8.9306e-04,  2.9630e-02,  5.4906e-02,\n",
      "        -2.5412e-03,  1.3340e-02,  4.0849e-02, -3.8963e-02,  1.7043e-02,\n",
      "        -2.8258e-02,  1.4415e-02,  1.2102e-02,  2.1172e-02, -1.2490e-02,\n",
      "         1.4809e-02,  3.0331e-03,  8.8036e-03, -1.6489e-02,  1.6877e-02,\n",
      "         1.0647e-02,  1.6862e-03,  3.0237e-02,  4.5282e-02,  1.2818e-03,\n",
      "        -3.4254e-02,  4.4912e-02, -1.5441e-02,  2.7577e-02,  5.4325e-03,\n",
      "        -1.7642e-02,  1.0978e-02, -2.1065e-02, -1.4703e-02, -4.4301e-03,\n",
      "        -1.9836e-02,  3.0947e-02,  5.9452e-03, -4.6393e-03, -1.8270e-02,\n",
      "        -5.2654e-03, -2.8889e-03, -6.6113e-03,  1.0855e-02, -3.7837e-02,\n",
      "         2.8421e-02,  1.1224e-02, -4.0770e-02,  2.1228e-02,  8.3407e-04,\n",
      "         1.3647e-02, -8.3652e-03, -2.8084e-02, -3.5809e-02, -3.2654e-03,\n",
      "         4.4403e-02, -8.8437e-04,  5.0059e-02,  8.9018e-03, -1.3286e-02,\n",
      "        -1.5440e-03, -1.1334e-02, -1.6655e-02,  1.7242e-02, -3.7783e-02,\n",
      "         7.4946e-03,  1.3057e-03, -1.9756e-02,  6.7960e-04, -9.6819e-03,\n",
      "        -1.1952e-02, -1.1641e-02, -3.4432e-02, -3.3279e-02, -2.0269e-02,\n",
      "         2.3908e-02, -2.3402e-02, -1.4121e-02,  3.3284e-02,  1.6427e-02,\n",
      "        -9.4096e-03,  1.6201e-02, -3.3781e-02,  2.1923e-03, -2.2353e-02,\n",
      "         2.0037e-02,  3.2727e-02,  1.8616e-02,  7.7108e-03, -1.8352e-02,\n",
      "        -4.3590e-02,  2.7115e-03, -1.7445e-02,  7.7576e-03, -1.2115e-02,\n",
      "         1.2357e-02, -3.9412e-02,  6.1363e-03, -2.5829e-02,  2.1287e-02,\n",
      "         3.6473e-02,  1.6665e-02, -2.7268e-02,  3.4857e-02,  1.2545e-03,\n",
      "        -2.4393e-02, -1.6876e-02,  1.9832e-02,  5.2836e-03,  2.4994e-02,\n",
      "        -4.1497e-02,  3.0811e-02,  1.8511e-02,  1.0048e-02,  3.7116e-02,\n",
      "        -3.8198e-04, -4.1669e-02,  1.1392e-02, -3.5299e-02, -3.7159e-02,\n",
      "         2.2479e-02, -1.5653e-03, -3.2378e-02,  4.7007e-02, -2.5268e-02,\n",
      "         1.8755e-02,  2.4399e-02, -2.1038e-02,  1.8494e-02,  1.4861e-02,\n",
      "         6.0487e-03,  1.9167e-02,  1.2446e-02, -1.1593e-02,  2.2803e-02,\n",
      "         1.0968e-02,  3.4517e-02, -2.3839e-02, -4.5100e-02, -2.0972e-02,\n",
      "         2.6979e-02, -5.2435e-03,  7.7106e-03,  2.5408e-02, -3.0608e-02,\n",
      "         1.6249e-02,  1.8247e-03, -1.7784e-02, -2.3989e-02, -1.6831e-02,\n",
      "         1.5120e-02, -2.3248e-02,  1.1673e-02,  4.1580e-02,  3.1134e-02,\n",
      "        -4.2236e-02, -4.1232e-03, -7.8598e-03, -3.0886e-02,  2.1759e-02,\n",
      "        -1.6913e-02, -8.4524e-03,  2.6422e-02, -3.0527e-02,  1.0312e-02,\n",
      "         2.2602e-02,  1.7208e-02,  2.5352e-03,  4.7649e-03, -9.3444e-03,\n",
      "        -9.8019e-03, -3.0532e-03,  2.2720e-02, -9.1312e-03, -7.2717e-04,\n",
      "        -3.6712e-02, -4.2214e-03, -2.0843e-02,  2.4917e-02,  1.9493e-02,\n",
      "        -1.6778e-02,  9.2138e-03,  4.0198e-03, -2.4332e-02, -2.3615e-02,\n",
      "        -1.9608e-03, -3.8880e-03, -9.8627e-03, -1.4579e-02,  1.5015e-02,\n",
      "        -2.7622e-03,  1.3031e-02, -7.6566e-03, -5.9041e-05,  5.3238e-03,\n",
      "        -3.2084e-02,  1.1244e-02,  9.9762e-03, -5.1334e-03, -2.1300e-02,\n",
      "         1.9938e-02, -3.3849e-02,  6.2270e-03, -5.1782e-03,  2.9644e-02,\n",
      "        -8.4892e-03, -2.6729e-03,  2.1796e-02, -1.0553e-02,  7.7147e-03,\n",
      "         4.1103e-02,  1.2092e-02, -2.5626e-02,  1.7872e-02,  2.0477e-02,\n",
      "        -1.9742e-02, -2.1289e-02, -1.5612e-02, -2.8018e-02, -3.4506e-02,\n",
      "         1.0558e-02, -2.5910e-02,  1.8283e-03,  3.9048e-02, -3.4889e-02,\n",
      "         3.1944e-02, -2.5436e-02,  1.2458e-02, -1.8837e-02, -1.4304e-02,\n",
      "         1.9952e-02, -1.1592e-02,  2.8469e-03, -3.0734e-02,  4.7830e-02,\n",
      "        -1.6987e-03, -2.8271e-02,  1.1281e-02, -5.9754e-03,  2.1483e-02,\n",
      "         2.9843e-02,  4.2917e-02, -2.9361e-02, -3.2170e-02,  3.0919e-02,\n",
      "        -1.3106e-02,  1.7577e-02, -9.5854e-03, -1.3684e-03, -1.7916e-02,\n",
      "        -1.6523e-02, -3.7099e-02, -9.8214e-03,  4.0964e-03,  3.6069e-02,\n",
      "         6.5628e-04, -1.3847e-02,  1.0138e-02,  1.0918e-02,  3.1355e-03,\n",
      "         1.6046e-02, -1.2819e-02,  3.8406e-02,  3.0315e-02, -4.6105e-03,\n",
      "         3.1159e-02, -3.0333e-02,  1.8294e-02,  2.5301e-02, -1.5740e-03,\n",
      "         1.0298e-02, -2.0407e-02,  2.0321e-02, -6.8375e-05, -4.4049e-02,\n",
      "        -1.0805e-02,  5.5464e-03,  5.0809e-03, -2.3813e-02, -3.9342e-03,\n",
      "         3.0230e-02,  1.1473e-02,  1.3812e-02, -1.4290e-02, -2.3431e-03,\n",
      "         1.7356e-02, -3.2904e-03, -5.0023e-03,  2.8947e-02,  2.1867e-02,\n",
      "        -1.2475e-03,  1.1276e-02, -1.8874e-02, -6.7696e-03, -3.3686e-02,\n",
      "         1.1874e-03, -9.9203e-03,  1.3061e-02,  9.0013e-03, -1.1224e-02,\n",
      "        -2.6142e-02, -2.2047e-02, -3.1235e-02,  1.1690e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.state_dict().items():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter value: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
