{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pickle\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exquisite acronym explanation (also sounds like lean):\n",
    "# R - recurrent\n",
    "# E - embedding\n",
    "# A - approximation\n",
    "# N - network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings mode\n",
    "model_file = fr\"./embedding_models/wiki_model_5_XL_vector.model\"\n",
    "embeddings_model = Word2Vec.load(model_file)\n",
    "vector_size = embeddings_model.vector_size\n",
    "window = embeddings_model.window\n",
    "\n",
    "# neural net settings\n",
    "context_length = 16\n",
    "input_size = vector_size + context_length * vector_size\n",
    "attention_heads = 4\n",
    "\n",
    "# dataset\n",
    "train_dataset_path = fr\"./datasets/wiki_dump_train.txt\"\n",
    "test_dataset_path = fr\"./datasets/wiki_dump_test.txt\"\n",
    "unique_examples_train = 4096# * 8 * 8\n",
    "unique_examples_test = 4096\n",
    "fluffed_up_size_train = unique_examples_train# * context_length\n",
    "fluffed_up_size_test = unique_examples_test# * context_length\n",
    "predicted_ram_usage = (fluffed_up_size_test + fluffed_up_size_train) * vector_size * 4 / 1000 / 1000 / 100\n",
    "\n",
    "# training\n",
    "epochs = 25 // 2#6\n",
    "lr = 0.00001\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "batch_size = 4#8#16#32#56#1024 * 1\n",
    "\n",
    "# pytorch\n",
    "run_device = torch.device(\"cuda\")\n",
    "storage_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"settings summary:\\n\")\n",
    "print(f\"--- embeddings model ---\\nvec size: {vector_size}\\nwindow: {window}\\n\")\n",
    "print(f\"--- neural network ---\\ncontext length: {context_length}\\nlayer input size: {input_size}\\n\")\n",
    "print(f\"--- dataset ---\\nunique train examples: {unique_examples_train}\\nfluffed up train size: {fluffed_up_size_train}\\nunique test examples: {unique_examples_test}\\nfluffed up test size: {fluffed_up_size_test}\\npredicted ram requirements: {predicted_ram_usage:.2f} GB\\n\")\n",
    "print(f\"--- training ---\\nepochs: {epochs}\\nlr: {lr}\\nbatch size: {batch_size}\\noptimizer: {optimizer}\\nloss: {loss}\\n\")\n",
    "print(f\"--- pytorch ---\\ndevice: {run_device}   |   {torch.cuda.get_device_properties(0).name}\\nVRAM capacity: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024:.2f} GB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_mechanism(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(attention_mechanism, self).__init__()\n",
    "        # Linear layers to project input to queries, keys and values\n",
    "        self.query = nn.Linear(vector_size, vector_size).to(run_device)\n",
    "        self.key = nn.Linear(vector_size, vector_size).to(run_device)\n",
    "        self.value = nn.Linear(vector_size, vector_size).to(run_device)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.out = nn.Linear(vector_size, vector_size).to(run_device)\n",
    "        \n",
    "        # Scaling factor\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([vector_size // attention_heads])).to(run_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(run_device)\n",
    "        \n",
    "        # Linear projections\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        batch_size, context_length, vector_size = x.shape\n",
    "        queries = queries.view(batch_size, context_length, attention_heads, vector_size // attention_heads).permute(0, 2, 1, 3)\n",
    "        keys = keys.view(batch_size, context_length, attention_heads, vector_size // attention_heads).permute(0, 2, 1, 3)\n",
    "        values = values.view(batch_size, context_length, attention_heads, vector_size // attention_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.scale\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "        \n",
    "        # Concatenate heads and pass through the final linear layer\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, context_length, vector_size)\n",
    "        output = self.out(attention_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each block takes in:\n",
    "# the output of the block before it (prev_block)\n",
    "# the original input into the network (fresh_input)\n",
    "# the original prompt of the user, not necessarily the same as original_input (<REMOVE>)\n",
    "# each block returns:\n",
    "# a vector of the same size as the embedding (which can be passed into the next block)\n",
    "class REAN_block(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(REAN_block, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, vector_size)\n",
    "\n",
    "    def forward(self, prev_block: torch.Tensor, fresh_input: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat((prev_block, fresh_input), dim=1)\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(REAN, self).__init__()\n",
    "        \n",
    "        #self.attention_mechanism = attention_mechanism()\n",
    "        \n",
    "        self.block1 = REAN_block(input_size)\n",
    "        self.block2 = REAN_block(input_size)\n",
    "        self.block3 = REAN_block(input_size)\n",
    "        self.block4 = REAN_block(input_size)\n",
    "        self.block5 = REAN_block(input_size)\n",
    "        self.block6 = REAN_block(input_size)\n",
    "        self.block7 = REAN_block(input_size)\n",
    "        self.block8 = REAN_block(input_size)\n",
    "\n",
    "    def forward(self, current_segment: torch.Tensor) -> torch.Tensor:\n",
    "        # keep an x's clone from the start so the blocks get fresh input\n",
    "        self.fresh_input = current_segment.clone()\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        #self.fresh_input = self.attention_mechanism(self.fresh_input).reshape(current_segment.shape[0], vector_size * context_length)\n",
    "        self.fresh_input = self.fresh_input.reshape(current_segment.shape[0], vector_size * context_length)\n",
    "        \n",
    "        # supplement the (non existant) previous block's output with zeros\n",
    "        current_segment = self.block1(torch.zeros((current_segment.shape[0], vector_size), dtype=torch.float32, device=run_device), self.fresh_input)\n",
    "        current_segment = self.block2(current_segment, self.fresh_input)\n",
    "        current_segment = self.block3(current_segment, self.fresh_input)\n",
    "        current_segment = self.block4(current_segment, self.fresh_input)\n",
    "        current_segment = self.block5(current_segment, self.fresh_input)\n",
    "        current_segment = self.block6(current_segment, self.fresh_input)\n",
    "        current_segment = self.block7(current_segment, self.fresh_input)\n",
    "        current_segment = self.block8(current_segment, self.fresh_input)\n",
    "        \n",
    "        return current_segment\n",
    "\n",
    "net = REAN(input_size)\n",
    "net.to(run_device)\n",
    "optimizer = optimizer(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_chunk(path: str, num_words: int, seek_start: int, sep: str = \" \") -> tuple[list, bool, int]:\n",
    "    \"\"\"\n",
    "    function to load a chunk of the dataset where the words are separated by \"sep\" into a list\n",
    "    \n",
    "    parameters:\n",
    "        path (str): path to the dataset txt file\n",
    "        num_words (int): number of words to load\n",
    "        seek_start (int): start char to pull words from\n",
    "        sep (str, optional): separator in the dataset, defaults to space \" \"\n",
    "    \n",
    "    returns:\n",
    "        list: list of strings (loaded words), is EOF hit, seek position to move 1 word forward\n",
    "    \"\"\"\n",
    "    \n",
    "    # some safety checks so later code looks cleaner\n",
    "    num_words = max(0, num_words)\n",
    "    seek_start = max(0, seek_start)\n",
    "    \n",
    "    words = []\n",
    "    current_word_idx = 0\n",
    "    word_buffer = \"\"\n",
    "    current_seek = seek_start\n",
    "    next_seek = 0\n",
    "    first_word_flag = True\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        file.seek(seek_start)\n",
    "        \n",
    "        # loop over all chars after seek_start\n",
    "        while True:\n",
    "            char = file.read(1)\n",
    "            current_seek += 1\n",
    "            \n",
    "            # end of file, return whatever has been collected immediately\n",
    "            if not char:\n",
    "                return words, True, next_seek\n",
    "            \n",
    "            # is a separator between words hit\n",
    "            if char == sep or char.isspace():\n",
    "                if word_buffer:\n",
    "                    if current_word_idx < num_words:\n",
    "                        words.append(word_buffer)\n",
    "                    \n",
    "                    current_word_idx += 1\n",
    "                    word_buffer = \"\"\n",
    "                \n",
    "                if current_word_idx >= num_words:\n",
    "                    break\n",
    "                \n",
    "                # the first word is covered, this is where the next chunk is going to be loaded from\n",
    "                if first_word_flag:\n",
    "                    first_word_flag = False\n",
    "                    next_seek = current_seek\n",
    "            else:\n",
    "                word_buffer += char\n",
    "\n",
    "    return words, False, next_seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence: list[str], model: Word2Vec, default: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    encodes all words in a given list to corresponding vectors in given model.\n",
    "    words not found in the model will be given a vector with \"default\" value\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        model (Word2Vec): model to use when encoding\n",
    "        default (int): fill vector with this value if word is not found in model\n",
    "    \n",
    "    returns:\n",
    "        np.array: 2d array with dim1 = len(sentence) and dim2 = model.vector_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate inital array with default values\n",
    "    vectorized = np.ones((len(sentence), model.vector_size)) * default\n",
    "    \n",
    "    # loop over every word in list\n",
    "    for current_word, current_word_idx in zip(sentence, range(len(sentence))):\n",
    "        # only add correct values if word is in model, otherwise leave as default\n",
    "        if current_word in model.wv:\n",
    "            vectorized[current_word_idx] *= 0\n",
    "            vectorized[current_word_idx] += model.wv[current_word]\n",
    "    \n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devectorize_sentence(vectorized_sentence: np.array, model: Word2Vec) -> list:\n",
    "    \"\"\"\n",
    "    decodes vectors into nearest word found in model\n",
    "    \n",
    "    parameters:\n",
    "        vectorized_sentence (np.array): 2d arrat with vectors of words to be decoded\n",
    "        model (Word2Vec): model to use when decoding\n",
    "    \n",
    "    returns:\n",
    "        list: list of strings (words) whos vectors most closely match those provided\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # go over all words and find closest match in model\n",
    "    for current_word in vectorized_sentence:\n",
    "        result.append(model.wv.similar_by_vector(current_word)[0][0])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(suspected_tensor: torch.tensor, target_length: int, default: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pads or truncates a given tensor along dim 0 to target_length with \"default\" as padding\n",
    "    \n",
    "    parameters:\n",
    "        suspected_tensor (torch.tensor): tensor to pad or truncate\n",
    "        target_length (int): target length of tensor\n",
    "        default (int): value to use for padding\n",
    "    \n",
    "    returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(suspected_tensor) < target_length:\n",
    "        # pad\n",
    "        suspected_tensor = torch.cat((torch.ones(target_length - len(suspected_tensor), suspected_tensor.shape[1], dtype=torch.float32, device=suspected_tensor.device) * default, suspected_tensor))\n",
    "    else:\n",
    "        # truncate\n",
    "        suspected_tensor = suspected_tensor[-target_length:]\n",
    "    \n",
    "    return suspected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence_for_net(sentence: list, model: Word2Vec, context_length: int, flatten: bool=True, used_device: torch.device=run_device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    turns a sentence (list of strings) into a tensor that can be fed directly into the network\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        model (Word2Vec): model to use when encoding sentence\n",
    "        context_length (int): length of context to consider when encoding, should be same as network's\n",
    "    \n",
    "    returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode sentence to np.array\n",
    "    vectorized = vectorize_sentence(sentence, model)\n",
    "    vectorized_tensor = torch.from_numpy(vectorized).to(used_device).to(torch.float32)\n",
    "    \n",
    "    # pad or truncate\n",
    "    vectorized_tensor = pad_or_truncate(vectorized_tensor, context_length)\n",
    "    \n",
    "    if flatten:\n",
    "        # flatten to fit into first fc layer of the net\n",
    "        vectorized_tensor = vectorized_tensor.flatten()\n",
    "    \n",
    "    return vectorized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(current_segment: list, net: REAN, embeddings_model: Word2Vec) -> str:\n",
    "    \"\"\"\n",
    "    uses the net and the model to predict the next word to fit the given sentence\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        net (GPT_like): net to use when predicting\n",
    "        model (Word2Vec): embedding model to use when encoding sentence\n",
    "    \n",
    "    returns:\n",
    "        str: predicted word\n",
    "    \"\"\"\n",
    "    encoded_segment = prepare_sentence_for_net(current_segment, embeddings_model, context_length, flatten=False)\n",
    "    \n",
    "    # run sentence\n",
    "    output = net(encoded_segment.unsqueeze(0))\n",
    "    \n",
    "    # add the net's vector to the end of the current segment\n",
    "    target = output + encoded_segment[-1]\n",
    "    \n",
    "    # decode most similar word to whatever net predicted\n",
    "    predicted_word = embeddings_model.wv.similar_by_vector(target.detach().squeeze(0).cpu().numpy())[0][0]\n",
    "    \n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(sentence: list, net: REAN, embeddings_model: Word2Vec, num_completions: int) -> list:\n",
    "    \"\"\"\n",
    "    predicts multiple words at the end of the given sentence\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        net (GPT_like): net to use when predicting\n",
    "        model (Word2Vec): embedding model to use when encoding sentence\n",
    "        num_completions (int): number of words to predict\n",
    "    \n",
    "    returns:\n",
    "        list: list of words to be appended to given sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_result = sentence\n",
    "    \n",
    "    for _ in tqdm(range(num_completions)):\n",
    "        # give the network the full context to work with, while only collecting the predicted part into the result\n",
    "        predicted_result.append(predict_word(predicted_result, net, embeddings_model))\n",
    "    \n",
    "    return predicted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN_dataset(Dataset):\n",
    "    def extrapolate_and_add_example(self, path: str, seek_start: int, context_length: int, embeddings_model: Word2Vec, append_context: list, append_target: list):\n",
    "        self.current_segment, self.eof, self.seek_start = load_dataset_chunk(path, context_length + 1, seek_start)\n",
    "        \n",
    "        self.context = prepare_sentence_for_net(self.current_segment[:-1], embeddings_model, context_length, flatten=False, used_device=storage_device)\n",
    "        self.target = prepare_sentence_for_net([self.current_segment[-1]], embeddings_model, 1, flatten=False, used_device=storage_device).squeeze(0)\n",
    "        \n",
    "        for cut in range(len(self.current_segment) - random.randint(context_length // 2, context_length)):\n",
    "            self.context = prepare_sentence_for_net(self.current_segment[:-1][cut:], embeddings_model, context_length, flatten=False, used_device=storage_device)\n",
    "            \n",
    "            self.diff = self.target - self.context[-1]\n",
    "        \n",
    "            append_context.append(self.context)\n",
    "            append_target.append(self.diff)\n",
    "        \n",
    "        return self.eof, self.seek_start\n",
    "        \n",
    "    def __init__(self, path, num_unique_examples, context_length, embeddings_model):\n",
    "        self.seek_start = 0\n",
    "        \n",
    "        self.current_segments = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in tqdm(range(num_unique_examples)):\n",
    "            self.eof, self.seek_start = self.extrapolate_and_add_example(path, self.seek_start, context_length, embeddings_model, self.current_segments, self.targets)\n",
    "            \n",
    "            if self.eof:\n",
    "                print(\"eof hit, early stop\")\n",
    "                print(f\"fluffed up size: {len(self.targets)}\")\n",
    "                return\n",
    "        \n",
    "        print(f\"fluffed up size: {len(self.targets)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.current_segments[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = REAN_dataset(train_dataset_path, unique_examples_train, context_length, embeddings_model)\n",
    "test_dataset = REAN_dataset(test_dataset_path, unique_examples_test, context_length, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_graph = []\n",
    "test_loss_graph = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    for current_segment, target in train_loader:\n",
    "        # move batch to gpu\n",
    "        current_segment = current_segment.to(run_device)\n",
    "        target = target.to(run_device)\n",
    "        \n",
    "        # train batch\n",
    "        optimizer.zero_grad()\n",
    "        train_outputs = net(current_segment)\n",
    "        train_loss_value = loss(train_outputs, target)\n",
    "        train_loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect performance metrics\n",
    "        train_loss_graph.append(train_loss_value.item())\n",
    "        \n",
    "    if epoch % 24 == 0:\n",
    "        with torch.no_grad():\n",
    "            for test_current_segment, test_target in test_loader:\n",
    "                # move to gpu\n",
    "                test_current_segment = test_current_segment.to(run_device)\n",
    "                test_target = test_target.to(run_device)\n",
    "                \n",
    "                test_outputs = net(test_current_segment)\n",
    "                test_loss_value = loss(test_outputs, test_target)\n",
    "                test_loss_graph.append(test_loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'no_attention_mech.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"was the first man\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(predict_sequence(sentence, net, embeddings_model, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
