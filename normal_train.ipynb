{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pickle\n",
    "import threading\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exquisite acronym explanation (also sounds like lean):\n",
    "# R - recurrent\n",
    "# E - embedding\n",
    "# A - approximation\n",
    "# N - network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings mode\n",
    "model_file = fr\"./embedding_models/wiki_model3.model\"\n",
    "embeddings_model = Word2Vec.load(model_file)\n",
    "\n",
    "vector_size = embeddings_model.vector_size\n",
    "window = embeddings_model.window\n",
    "\n",
    "# neural net settings\n",
    "context_length = 16\n",
    "input_size = context_length * vector_size + vector_size # account for previous block and current fresh inputs\n",
    "\n",
    "# dataset\n",
    "train_dataset_path = fr\"./datasets/wiki_dump_train.txt\"\n",
    "test_dataset_path = fr\"./datasets/wiki_dump_test.txt\"\n",
    "\n",
    "unique_examples_train = 1024 * 8 * 6 * 2\n",
    "unique_examples_test = 4096\n",
    "\n",
    "# training\n",
    "train_epochs = 32\n",
    "warmup_epochs = 4\n",
    "\n",
    "initial_warmup_lr = 0.000001\n",
    "final_warmup_lr = 0.0001\n",
    "initial_train_lr = 0.0001\n",
    "final_train_lr = 0.0000001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "epsilon = 0.00000001\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "train_optimizer = torch.optim.Adam\n",
    "warmup_optimizer = torch.optim.Adam\n",
    "\n",
    "train_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "warmup_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "\n",
    "train_batch_size = 200\n",
    "test_batch_size = 200 # smaller specifically to get more datapoints per test run\n",
    "test_loop_epoch = 4\n",
    "\n",
    "plot_graphs = True\n",
    "\n",
    "# pytorch\n",
    "run_device = torch.device(\"cuda\")\n",
    "storage_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN_block(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(REAN_block, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, vector_size)\n",
    "\n",
    "    def forward(self, prev_block: torch.Tensor, fresh_input: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat((prev_block, fresh_input), dim=1)\n",
    "        \n",
    "        x = x / 5\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        \n",
    "        x = x * 5\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\CONDA_VENV\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "class REAN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(REAN, self).__init__()\n",
    "        \n",
    "        self.block1 = REAN_block(input_size)\n",
    "        self.block2 = REAN_block(input_size)\n",
    "        self.block3 = REAN_block(input_size)\n",
    "        self.block4 = REAN_block(input_size)\n",
    "        self.block5 = REAN_block(input_size)\n",
    "        self.block6 = REAN_block(input_size)\n",
    "        self.block7 = REAN_block(input_size)\n",
    "        self.block8 = REAN_block(input_size)\n",
    "        self.block9 = REAN_block(input_size)\n",
    "        self.block10 = REAN_block(input_size)\n",
    "        self.block11 = REAN_block(input_size)\n",
    "        self.block12 = REAN_block(input_size)\n",
    "\n",
    "    def forward(self, current_segment: torch.Tensor) -> torch.Tensor:\n",
    "        # keep an x's clone from the start so the blocks get fresh input\n",
    "        self.fresh_input = current_segment.clone()\n",
    "        \n",
    "        self.fresh_input = self.fresh_input.reshape(current_segment.shape[0], vector_size * context_length)\n",
    "        \n",
    "        # supplement the (currently non existant) previous block's output with the latest word\n",
    "        current_segment = self.block1(current_segment[:, -1], self.fresh_input)\n",
    "        current_segment = self.block2(current_segment, self.fresh_input)\n",
    "        current_segment = self.block3(current_segment, self.fresh_input)\n",
    "        current_segment = self.block4(current_segment, self.fresh_input)\n",
    "        current_segment = self.block5(current_segment, self.fresh_input)\n",
    "        current_segment = self.block6(current_segment, self.fresh_input)\n",
    "        current_segment = self.block7(current_segment, self.fresh_input)\n",
    "        current_segment = self.block8(current_segment, self.fresh_input)\n",
    "        current_segment = self.block9(current_segment, self.fresh_input)\n",
    "        current_segment = self.block10(current_segment, self.fresh_input)\n",
    "        current_segment = self.block11(current_segment, self.fresh_input)\n",
    "        current_segment = self.block12(current_segment, self.fresh_input)\n",
    "        \n",
    "        return current_segment\n",
    "\n",
    "net = REAN(input_size)\n",
    "net.to(run_device)\n",
    "\n",
    "train_optimizer = train_optimizer(net.parameters(), lr=initial_train_lr, betas=(beta1, beta2), eps=epsilon)\n",
    "warmup_optimizer = warmup_optimizer(net.parameters(), lr=initial_warmup_lr, betas=(beta1, beta2), eps=epsilon)\n",
    "\n",
    "train_scheduler = train_scheduler(train_optimizer, T_max=train_epochs, eta_min=final_train_lr)\n",
    "warmup_scheduler = warmup_scheduler(warmup_optimizer, T_max=warmup_epochs, eta_min=final_warmup_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence: list[str], model: Word2Vec, default: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    encodes all words in a given list to corresponding vectors in given model.\n",
    "    words not found in the model will be given a vector with \"default\" value\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        model (Word2Vec): model to use when encoding\n",
    "        default (int): fill vector with this value if word is not found in model\n",
    "    \n",
    "    returns:\n",
    "        np.array: 2d array with dim1 = len(sentence) and dim2 = model.vector_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate inital array with default values\n",
    "    vectorized = np.ones((len(sentence), model.vector_size)) * default\n",
    "    \n",
    "    # loop over every word in list\n",
    "    for current_word, current_word_idx in zip(sentence, range(len(sentence))):\n",
    "        # only add correct values if word is in model, otherwise leave as default\n",
    "        if current_word in model.wv:\n",
    "            vectorized[current_word_idx] *= 0\n",
    "            vectorized[current_word_idx] += model.wv[current_word]\n",
    "    \n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devectorize_sentence(vectorized_sentence: np.array, model: Word2Vec) -> list:\n",
    "    \"\"\"\n",
    "    decodes vectors into nearest word found in model\n",
    "    \n",
    "    parameters:\n",
    "        vectorized_sentence (np.array): 2d arrat with vectors of words to be decoded\n",
    "        model (Word2Vec): model to use when decoding\n",
    "    \n",
    "    returns:\n",
    "        list: list of strings (words) whos vectors most closely match those provided\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # go over all words and find closest match in model\n",
    "    for current_word in vectorized_sentence:\n",
    "        result.append(model.wv.similar_by_vector(current_word)[0][0])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(suspected_tensor: torch.tensor, target_length: int, default: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pads or truncates a given tensor along dim 0 to target_length with \"default\" as padding\n",
    "    \n",
    "    parameters:\n",
    "        suspected_tensor (torch.tensor): tensor to pad or truncate\n",
    "        target_length (int): target length of tensor\n",
    "        default (int): value to use for padding\n",
    "    \n",
    "    returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(suspected_tensor) < target_length:\n",
    "        # pad\n",
    "        suspected_tensor = torch.cat((torch.ones(target_length - len(suspected_tensor), suspected_tensor.shape[1], dtype=torch.float32, device=suspected_tensor.device) * default, suspected_tensor))\n",
    "    else:\n",
    "        # truncate\n",
    "        suspected_tensor = suspected_tensor[-target_length:]\n",
    "    \n",
    "    return suspected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence_for_net(sentence: list, model: Word2Vec, context_length: int, flatten: bool = True, used_device: torch.device = run_device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Turns a sentence (list of strings) into a tensor that can be fed directly into the network\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        model (Word2Vec): model to use when encoding sentence\n",
    "        context_length (int): length of context to consider when encoding, should be same as network's\n",
    "        flatten (bool): whether to flatten the tensor to fit into the first fc layer of the net\n",
    "        used_device (torch.device): the device to use for the tensor\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode sentence to numpy array\n",
    "    vectorized = vectorize_sentence(sentence, model)\n",
    "    \n",
    "    # Directly create the tensor on the target device\n",
    "    vectorized_tensor = torch.tensor(vectorized, dtype=torch.float32, device=used_device)\n",
    "    \n",
    "    # Pad or truncate\n",
    "    vectorized_tensor = pad_or_truncate(vectorized_tensor, context_length)\n",
    "    \n",
    "    if flatten:\n",
    "        # Flatten to fit into the first fully connected layer of the net\n",
    "        vectorized_tensor = vectorized_tensor.flatten()\n",
    "    \n",
    "    return vectorized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(current_segment: list, net: REAN, embeddings_model: Word2Vec) -> str:\n",
    "    \"\"\"\n",
    "    uses the net and the model to predict the next word to fit the given sentence\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        net (GPT_like): net to use when predicting\n",
    "        model (Word2Vec): embedding model to use when encoding sentence\n",
    "    \n",
    "    returns:\n",
    "        str: predicted word\n",
    "    \"\"\"\n",
    "    encoded_segment = prepare_sentence_for_net(current_segment, embeddings_model, context_length, flatten=False)\n",
    "    \n",
    "    # run sentence\n",
    "    output = net(encoded_segment.unsqueeze(0))\n",
    "    \n",
    "    # add the net's vector to the end of the current segment\n",
    "    target = output + encoded_segment[-1]\n",
    "    \n",
    "    # decode most similar word to whatever net predicted\n",
    "    predicted_word = embeddings_model.wv.similar_by_vector(target.detach().squeeze(0).cpu().numpy())[0][0]\n",
    "    \n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(sentence: list, net: REAN, embeddings_model: Word2Vec, num_completions: int) -> list:\n",
    "    \"\"\"\n",
    "    predicts multiple words at the end of the given sentence\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        net (GPT_like): net to use when predicting\n",
    "        model (Word2Vec): embedding model to use when encoding sentence\n",
    "        num_completions (int): number of words to predict\n",
    "    \n",
    "    returns:\n",
    "        list: list of words to be appended to given sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_result = sentence\n",
    "    \n",
    "    for _ in tqdm(range(num_completions)):\n",
    "        # give the network the full context to work with, while only collecting the predicted part into the result\n",
    "        predicted_result.append(predict_word(predicted_result, net, embeddings_model))\n",
    "    \n",
    "    return predicted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN_dataset(Dataset):\n",
    "    def load_dataset_chunk(self, path: str, num_words: int, seek_start: int, sep: str = \" \") -> tuple[list, bool, int]:\n",
    "        \"\"\"\n",
    "        function to load a chunk of the dataset where the words are separated by \"sep\" into a list\n",
    "        \n",
    "        parameters:\n",
    "            path (str): path to the dataset txt file\n",
    "            num_words (int): number of words to load\n",
    "            seek_start (int): start char to pull words from\n",
    "            sep (str, optional): separator in the dataset, defaults to space \" \"\n",
    "        \n",
    "        returns:\n",
    "            list: list of strings (loaded words), is EOF hit, seek position to move 1 word forward\n",
    "        \"\"\"\n",
    "        \n",
    "        # some safety checks so later code looks cleaner\n",
    "        self.num_words = max(0, num_words)\n",
    "        self.seek_start = max(0, seek_start)\n",
    "        \n",
    "        self.words = []\n",
    "        self.current_word_idx = 0\n",
    "        self.word_buffer = \"\"\n",
    "        self.current_seek = seek_start\n",
    "        self.next_seek = 0\n",
    "        self.first_word_flag = True\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as self.file:\n",
    "            self.file.seek(seek_start)\n",
    "            \n",
    "            # loop over all chars after seek_start\n",
    "            while True:\n",
    "                self.char = self.file.read(1)\n",
    "                self.current_seek += 1\n",
    "                \n",
    "                # end of file, return whatever has been collected immediately\n",
    "                if not self.char:\n",
    "                    return self.words, True, self.next_seek\n",
    "                \n",
    "                # is a separator between words hit\n",
    "                if self.char == sep or self.char.isspace():\n",
    "                    if self.word_buffer:\n",
    "                        if self.current_word_idx < self.num_words:\n",
    "                            self.words.append(self.word_buffer)\n",
    "                        \n",
    "                        self.current_word_idx += 1\n",
    "                        self.word_buffer = \"\"\n",
    "                    \n",
    "                    if self.current_word_idx >= self.num_words:\n",
    "                        break\n",
    "                    \n",
    "                    # the first word is covered, this is where the next chunk is going to be loaded from\n",
    "                    if self.first_word_flag:\n",
    "                        self.first_word_flag = False\n",
    "                        self.next_seek = self.current_seek\n",
    "                else:\n",
    "                    self.word_buffer += self.char\n",
    "\n",
    "        return self.words, False, self.next_seek\n",
    "    \n",
    "    def init_segment_coords(self, num_unique_examples: int, path: str, context_length: int) -> list[int]:\n",
    "        self.seek_starts = [0]\n",
    "        \n",
    "        for _ in tqdm(range(num_unique_examples)):\n",
    "            _, self.eof, self.next_seek_start = self.load_dataset_chunk(path, context_length, self.seek_starts[-1])\n",
    "            \n",
    "            if self.eof:\n",
    "                print(\"eof hit, early stop\")\n",
    "                print(\"dataset has been generated succesfully, even though it might be smaller than you expect, size: \", len(self.seek_starts))\n",
    "                return self.seek_starts[:-1]\n",
    "            \n",
    "            self.seek_starts.append(self.next_seek_start)\n",
    "        \n",
    "        print(\"dataset has been generated succesfully, size: \", len(self.seek_starts))\n",
    "        \n",
    "        return self.seek_starts[:-1]\n",
    "    \n",
    "    def retrieve_example(self, path: str, context_length: int, seek_start: int, embeddings_model: Word2Vec) -> tuple[np.ndarray, np.ndarray]:\n",
    "        self.segment, _, _ = self.load_dataset_chunk(path, context_length + 1, seek_start)\n",
    "        self.encoded_segment = prepare_sentence_for_net(self.segment, embeddings_model, len(self.segment), flatten=False)\n",
    "        \n",
    "        self.context = self.encoded_segment[:-1]\n",
    "        self.target = self.encoded_segment[-1]\n",
    "        \n",
    "        self.target_diff = self.target - self.context[-1]\n",
    "        \n",
    "        return self.context, self.target_diff\n",
    "    \n",
    "    def increase_dataset_cut(self, dataset_cut: list, required_size: int, coords: list, path: str, context_length: int, embeddings_model: Word2Vec):\n",
    "        while len(dataset_cut) < required_size:\n",
    "            self.current_example = self.retrieve_example(path, context_length, coords[self.cut_read_start % len(coords)], embeddings_model)\n",
    "            dataset_cut.append(self.current_example)\n",
    "            self.cut_read_start += 1\n",
    "        \n",
    "    def __init__(self, path, num_unique_examples, context_length, embeddings_model, dataset_cut_buffer_size):\n",
    "        # transfer to object wide variables\n",
    "        self.path = path\n",
    "        self.context_length = context_length\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.num_unique_examples = num_unique_examples\n",
    "        self.dataset_cut_buffer_size = dataset_cut_buffer_size\n",
    "        \n",
    "        self.current_dataset_cut = []\n",
    "        self.cut_read_start = 0\n",
    "        \n",
    "        # generate initial coords for each example for the net (from the start of the dataset to the end)\n",
    "        self.coords = self.init_segment_coords(self.num_unique_examples, self.path, self.context_length)\n",
    "        \n",
    "        # apply shuffle here (instead of dataloader shuffle)\n",
    "        random.shuffle(self.coords)\n",
    "        \n",
    "        # make an initial \"commit\" to the dataset\n",
    "        self.increase_dataset_cut(self.current_dataset_cut, self.dataset_cut_buffer_size, self.coords, self.path, self.context_length, self.embeddings_model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        self.requested_item = self.current_dataset_cut[0]\n",
    "        self.current_dataset_cut.pop(0)\n",
    "        \n",
    "        # no threading version:\n",
    "        self.increase_dataset_cut(self.current_dataset_cut, self.dataset_cut_buffer_size, self.coords, self.path, self.context_length, self.embeddings_model)\n",
    "        \n",
    "        return self.requested_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1502/98304 [00:00<00:13, 7171.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98304/98304 [00:12<00:00, 7943.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has been generated succesfully, size:  98305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:00<00:00, 8045.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has been generated succesfully, size:  4097\n"
     ]
    }
   ],
   "source": [
    "train_dataset = REAN_dataset(train_dataset_path, unique_examples_train, context_length, embeddings_model, train_batch_size * 8)\n",
    "test_dataset = REAN_dataset(test_dataset_path, unique_examples_test, context_length, embeddings_model, test_batch_size * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !NOTE! shuffle is set to false because the dataset class itself is shuffling the data\n",
    "# this is done to make loading chunks of the dataset easier\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_graph = []\n",
    "test_loss_graph = []\n",
    "learning_rate_graph = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(warmup_epochs)):\n",
    "    # training loop\n",
    "    for current_segment, target in train_loader:\n",
    "        # move batch to gpu\n",
    "        #current_segment = current_segment.to(run_device)\n",
    "        #target = target.to(run_device)\n",
    "        \n",
    "        # train batch\n",
    "        warmup_optimizer.zero_grad()\n",
    "        train_outputs = net(current_segment)\n",
    "        train_loss_value = loss(train_outputs, target)\n",
    "        train_loss_value.backward()\n",
    "        warmup_optimizer.step()\n",
    "        \n",
    "        # collect performance metrics\n",
    "        train_loss_graph.append(train_loss_value.item())\n",
    "    \n",
    "    # eval loop\n",
    "    if epoch % test_loop_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            for test_current_segment, test_target in test_loader:\n",
    "                # move batch to gpu\n",
    "                #test_current_segment = test_current_segment.to(run_device)\n",
    "                #test_target = test_target.to(run_device)\n",
    "                \n",
    "                # run test\n",
    "                test_outputs = net(test_current_segment)\n",
    "                test_loss_value = loss(test_outputs, test_target)\n",
    "                \n",
    "                # collect performance metrics\n",
    "                test_loss_graph.append(test_loss_value.item())\n",
    "    \n",
    "    warmup_scheduler.step()\n",
    "    \n",
    "    # collect perforamce metrics\n",
    "    learning_rate_graph.append(warmup_optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # plot everything\n",
    "    if plot_graphs:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Plot training loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_loss_graph, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot testing loss\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(range(0, len(test_loss_graph) * 3, 3), test_loss_graph, label='Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot learning rate\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(learning_rate_graph, label='Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(train_epochs)):\n",
    "    # training loop\n",
    "    for current_segment, target in train_loader:\n",
    "        # move batch to gpu\n",
    "        current_segment = current_segment.to(run_device)\n",
    "        target = target.to(run_device)\n",
    "        \n",
    "        # train batch\n",
    "        train_optimizer.zero_grad()\n",
    "        train_outputs = net(current_segment)\n",
    "        train_loss_value = loss(train_outputs, target)\n",
    "        train_loss_value.backward()\n",
    "        train_optimizer.step()\n",
    "        \n",
    "        # collect performance metrics\n",
    "        train_loss_graph.append(train_loss_value.item())\n",
    "    \n",
    "    # eval loop\n",
    "    if epoch % test_loop_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            for test_current_segment, test_target in test_loader:\n",
    "                # move batch to gpu\n",
    "                test_current_segment = test_current_segment.to(run_device)\n",
    "                test_target = test_target.to(run_device)\n",
    "                \n",
    "                # run test\n",
    "                test_outputs = net(test_current_segment)\n",
    "                test_loss_value = loss(test_outputs, test_target)\n",
    "                \n",
    "                # collect performance metrics\n",
    "                test_loss_graph.append(test_loss_value.item())\n",
    "    \n",
    "    train_scheduler.step()\n",
    "    \n",
    "    # collect perforamce metrics\n",
    "    learning_rate_graph.append(train_optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # plot everything\n",
    "    if plot_graphs:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Plot training loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_loss_graph, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot testing loss\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(range(0, len(test_loss_graph) * 3, 3), test_loss_graph, label='Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot learning rate\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(learning_rate_graph, label='Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), 'no_attention_mech.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"there were no available\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'there were no available available available available available available available available available available available available available available available available available available available available available available available available available available available available available available available available available'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(predict_sequence(sentence, net, embeddings_model, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
