{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset settings\n",
    "dataset_path = fr\"./datasets/wiki_dump.txt\"\n",
    "dataset_chunk_size = 256\n",
    "chunks_per_corpus = 6\n",
    "\n",
    "# word2vec settings\n",
    "vector_size = 1024\n",
    "epochs = 16\n",
    "window = 16\n",
    "min_count = 1\n",
    "workers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_chunk(path: str, num_words: int, seek_start: int, sep: str = \" \") -> tuple[list, bool, int]:\n",
    "    \"\"\"\n",
    "    function to load a chunk of the dataset where the words are separated by \"sep\" into a list\n",
    "    \n",
    "    parameters:\n",
    "        path (str): path to the dataset txt file\n",
    "        num_words (int): number of words to load\n",
    "        seek_start (int): start char to pull words from\n",
    "        sep (str, optional): separator in the dataset, defaults to space \" \"\n",
    "    \n",
    "    returns:\n",
    "        list: list of strings (loaded words), is EOF hit, seek position to move 1 word forward\n",
    "    \"\"\"\n",
    "    \n",
    "    # some safety checks so later code looks cleaner ;)\n",
    "    num_words = max(0, num_words)\n",
    "    seek_start = max(0, seek_start)\n",
    "    \n",
    "    words = []\n",
    "    current_word_idx = 0\n",
    "    word_buffer = \"\"\n",
    "    current_seek = seek_start\n",
    "    next_seek = 0\n",
    "    first_word_flag = True\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        file.seek(seek_start)\n",
    "        \n",
    "        # loop over all chars after seek_start\n",
    "        while True:\n",
    "            char = file.read(1)\n",
    "            current_seek += 1\n",
    "            \n",
    "            # end of file, return whatever has been collected immediately\n",
    "            if not char:\n",
    "                return words, True, next_seek\n",
    "            \n",
    "            # is a separator between words hit\n",
    "            if char == sep or char.isspace():\n",
    "                if word_buffer:\n",
    "                    if current_word_idx < num_words:\n",
    "                        words.append(word_buffer)\n",
    "                    \n",
    "                    current_word_idx += 1\n",
    "                    word_buffer = \"\"\n",
    "                \n",
    "                if current_word_idx >= num_words:\n",
    "                    break\n",
    "                \n",
    "                # the first word is covered, this is where the next chunk is going to be loaded from\n",
    "                if first_word_flag:\n",
    "                    first_word_flag = False\n",
    "                    next_seek = current_seek\n",
    "            else:\n",
    "                word_buffer += char\n",
    "\n",
    "    return words, False, next_seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(dataset_path, dataset_start, dataset_chunk_size, chunks_per_corpus):\n",
    "    chunk, eof, seek = [], False, 0\n",
    "    corpus = []\n",
    "    \n",
    "    for _ in range(chunks_per_corpus):\n",
    "        chunk, eof, seek = load_dataset_chunk(dataset_path, dataset_chunk_size, dataset_start)\n",
    "        corpus.append(\" \".join(chunk))\n",
    "        dataset_start = seek\n",
    "        \n",
    "        if eof:\n",
    "            return corpus\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    processed_corpus = [simple_preprocess(doc) for doc in corpus]\n",
    "\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_chunk_train(model, dataset_path, dataset_chunk_size, chunks_per_corpus, train_corpuses, model_epochs=epochs):\n",
    "    chunk_start = 0\n",
    "    initial_corpus = load_corpus(dataset_path, chunk_start, dataset_chunk_size, chunks_per_corpus)\n",
    "    processed_initial_corpus = preprocess_corpus(initial_corpus)\n",
    "    \n",
    "    # Build vocabulary with the first chunk\n",
    "    model.build_vocab(processed_initial_corpus)\n",
    "    \n",
    "    chunk_start += dataset_chunk_size * chunks_per_corpus\n",
    "    \n",
    "    # Train with the first chunk\n",
    "    model.train(processed_initial_corpus, total_examples=model.corpus_count, epochs=model_epochs)\n",
    "    \n",
    "    for current_corpus in tqdm(range(train_corpuses - 1)):  # already trained on the first chunk\n",
    "        corpus = load_corpus(dataset_path, chunk_start, dataset_chunk_size, chunks_per_corpus)\n",
    "        chunk_start += dataset_chunk_size * chunks_per_corpus\n",
    "        \n",
    "        processed_corpus = preprocess_corpus(corpus)\n",
    "        \n",
    "        # Update vocabulary with the new chunk\n",
    "        model.build_vocab(processed_corpus, update=True)\n",
    "        \n",
    "        # Train with the new chunk\n",
    "        model.train(processed_corpus, total_examples=model.corpus_count, epochs=model_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model initially\n",
    "model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 290/16383 [01:00<1:16:09,  3.52it/s]"
     ]
    }
   ],
   "source": [
    "# train\n",
    "multi_chunk_train(model, dataset_path, dataset_chunk_size, chunks_per_corpus, 4096 * 2 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "model.save(\"wiki_model_7_mini_window.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similar_by_word(\"mission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
