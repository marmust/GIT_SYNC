{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pickle\n",
    "import threading\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exquisite acronym explanation (also sounds like lean):\n",
    "# R - recurrent\n",
    "# E - embedding\n",
    "# A - approximation\n",
    "# N - network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings mode\n",
    "model_file = fr\"./embedding_models/wiki_model3.model\"\n",
    "embeddings_model = Word2Vec.load(model_file)\n",
    "vector_size = embeddings_model.vector_size\n",
    "window = embeddings_model.window\n",
    "\n",
    "# neural net settings\n",
    "segment_size = 8\n",
    "input_size = vector_size * segment_size\n",
    "output_size = vector_size * segment_size\n",
    "\n",
    "# dataset\n",
    "train_dataset_path = fr\"./datasets/wiki_dump_train.txt\"\n",
    "test_dataset_path = fr\"./datasets/wiki_dump_test.txt\"\n",
    "unique_examples_train = 4096 * 8 * 3\n",
    "unique_examples_test = 4096\n",
    "lerp_steps = 3\n",
    "\n",
    "# training\n",
    "epochs = 128\n",
    "initial_lr = 0.0001\n",
    "final_lr = 0.00001\n",
    "gamma = (final_lr / initial_lr) ** (1 / epochs)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "batch_size = 1024#8#16#32#56#1024 * 1\n",
    "\n",
    "# pytorch\n",
    "run_device = torch.device(\"cuda\")\n",
    "storage_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the stats of run_device\n",
    "run_device_stats = torch.cuda.get_device_properties(run_device)\n",
    "print(run_device_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class REAN_block(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(REAN_block, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, output_size)\n",
    "\n",
    "    def forward(self, prev_block: torch.Tensor, fresh_input: torch.Tensor) -> torch.Tensor:\n",
    "        # flatten for fc layers\n",
    "        #if not prev_block.is_contiguous():\n",
    "        #    prev_block = prev_block.contiguous()\n",
    "        #if not fresh_input.is_contiguous():\n",
    "        #    fresh_input = fresh_input.contiguous()\n",
    "        \n",
    "        batch_given = prev_block.shape[0]\n",
    "        \n",
    "        prev_block = prev_block.view(batch_given, segment_size * vector_size)\n",
    "        fresh_input = fresh_input.view(batch_given, segment_size * vector_size)\n",
    "        \n",
    "        x = torch.cat((prev_block, fresh_input), dim=1)\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # reshape so next block will have correct shape\n",
    "        x = x.view(batch_given, segment_size, vector_size)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intake dim: (batch, segment_size, vector_size) x 1\n",
    "# output dim: (batch, segment_size, vector_size) x 1\n",
    "class REAN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(REAN, self).__init__()\n",
    "        \n",
    "        self.block1 = REAN_block(input_size, output_size)\n",
    "        self.block2 = REAN_block(input_size, output_size)\n",
    "        self.block3 = REAN_block(input_size, output_size)\n",
    "        self.block4 = REAN_block(input_size, output_size)\n",
    "        self.block5 = REAN_block(input_size, output_size)\n",
    "        self.block6 = REAN_block(input_size, output_size)\n",
    "        self.block7 = REAN_block(input_size, output_size)\n",
    "        self.block8 = REAN_block(input_size, output_size)\n",
    "\n",
    "    def forward(self, current_segment: torch.Tensor) -> torch.Tensor:\n",
    "        # keep x's clone from the start so the blocks get fresh input\n",
    "        self.fresh_input = current_segment.clone()\n",
    "        \n",
    "        current_segment = self.block1(current_segment, self.fresh_input)\n",
    "        current_segment = self.block2(current_segment, self.fresh_input)\n",
    "        current_segment = self.block3(current_segment, self.fresh_input)\n",
    "        current_segment = self.block4(current_segment, self.fresh_input)\n",
    "        current_segment = self.block5(current_segment, self.fresh_input)\n",
    "        current_segment = self.block6(current_segment, self.fresh_input)\n",
    "        current_segment = self.block7(current_segment, self.fresh_input)\n",
    "        current_segment = self.block8(current_segment, self.fresh_input)\n",
    "        \n",
    "        return current_segment\n",
    "\n",
    "net = REAN(input_size)\n",
    "net.to(run_device)\n",
    "optimizer = optimizer(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(start: float, end: float, progress: float) -> float:\n",
    "    return start + progress * (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_chunk(path: str, num_words: int, seek_start: int, sep: str = \" \") -> tuple[list, bool, int]:\n",
    "    \"\"\"\n",
    "    function to load a chunk of the dataset where the words are separated by \"sep\" into a list\n",
    "    \n",
    "    parameters:\n",
    "        path (str): path to the dataset txt file\n",
    "        num_words (int): number of words to load\n",
    "        seek_start (int): start char to pull words from\n",
    "        sep (str, optional): separator in the dataset, defaults to space \" \"\n",
    "    \n",
    "    returns:\n",
    "        list: list of strings (loaded words), is EOF hit, seek position to move 1 word forward\n",
    "    \"\"\"\n",
    "    \n",
    "    # some safety checks so later code looks cleaner\n",
    "    num_words = max(0, num_words)\n",
    "    seek_start = max(0, seek_start)\n",
    "    \n",
    "    words = []\n",
    "    current_word_idx = 0\n",
    "    word_buffer = \"\"\n",
    "    current_seek = seek_start\n",
    "    next_seek = 0\n",
    "    first_word_flag = True\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        file.seek(seek_start)\n",
    "        \n",
    "        # loop over all chars after seek_start\n",
    "        while True:\n",
    "            char = file.read(1)\n",
    "            current_seek += 1\n",
    "            \n",
    "            # end of file, return whatever has been collected immediately\n",
    "            if not char:\n",
    "                return words, True, next_seek\n",
    "            \n",
    "            # is a separator between words hit\n",
    "            if char == sep or char.isspace():\n",
    "                if word_buffer:\n",
    "                    if current_word_idx < num_words:\n",
    "                        words.append(word_buffer)\n",
    "                    \n",
    "                    current_word_idx += 1\n",
    "                    word_buffer = \"\"\n",
    "                \n",
    "                if current_word_idx >= num_words:\n",
    "                    break\n",
    "                \n",
    "                # the first word is covered, this is where the next chunk is going to be loaded from\n",
    "                if first_word_flag:\n",
    "                    first_word_flag = False\n",
    "                    next_seek = current_seek\n",
    "            else:\n",
    "                word_buffer += char\n",
    "\n",
    "    return words, False, next_seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence: list[str], model: Word2Vec, default: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    encodes all words in a given list to corresponding vectors in given model.\n",
    "    words not found in the model will be given a vector with \"default\" value\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        model (Word2Vec): model to use when encoding\n",
    "        default (int): fill vector with this value if word is not found in model\n",
    "    \n",
    "    returns:\n",
    "        np.array: 2d array with dim1 = len(sentence) and dim2 = model.vector_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate inital array with default values\n",
    "    vectorized = np.ones((len(sentence), model.vector_size)) * default\n",
    "    \n",
    "    # loop over every word in list\n",
    "    for current_word, current_word_idx in zip(sentence, range(len(sentence))):\n",
    "        # only add correct values if word is in model, otherwise leave as default\n",
    "        if current_word in model.wv:\n",
    "            vectorized[current_word_idx] *= 0\n",
    "            vectorized[current_word_idx] += model.wv[current_word]\n",
    "    \n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devectorize_sentence(vectorized_sentence: np.array, model: Word2Vec) -> list:\n",
    "    \"\"\"\n",
    "    decodes vectors into nearest word found in model\n",
    "    \n",
    "    parameters:\n",
    "        vectorized_sentence (np.array): 2d arrat with vectors of words to be decoded\n",
    "        model (Word2Vec): model to use when decoding\n",
    "    \n",
    "    returns:\n",
    "        list: list of strings (words) whos vectors most closely match those provided\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # go over all words and find closest match in model\n",
    "    for current_word in vectorized_sentence:\n",
    "        result.append(model.wv.similar_by_vector(current_word)[0][0])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(suspected_tensor: torch.tensor, target_length: int, default: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pads or truncates a given tensor along dim 0 to target_length with \"default\" as padding\n",
    "    \n",
    "    parameters:\n",
    "        suspected_tensor (torch.tensor): tensor to pad or truncate\n",
    "        target_length (int): target length of tensor\n",
    "        default (int): value to use for padding\n",
    "    \n",
    "    returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(suspected_tensor) < target_length:\n",
    "        # pad\n",
    "        suspected_tensor = torch.cat((torch.ones(target_length - len(suspected_tensor), suspected_tensor.shape[1], dtype=torch.float32, device=suspected_tensor.device) * default, suspected_tensor))\n",
    "    else:\n",
    "        # truncate\n",
    "        suspected_tensor = suspected_tensor[-target_length:]\n",
    "    \n",
    "    return suspected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence_for_net(sentence: list, model: Word2Vec, context_length: int, flatten: bool=True, used_device: torch.device=run_device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    turns a sentence (list of strings) into a tensor that can be fed directly into the network\n",
    "    \n",
    "    parameters:\n",
    "        sentence (list): list of strings (words)\n",
    "        model (Word2Vec): model to use when encoding sentence\n",
    "        context_length (int): length of context to consider when encoding, should be same as network's\n",
    "    \n",
    "    returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode sentence to np.array\n",
    "    vectorized = vectorize_sentence(sentence, model)\n",
    "    vectorized_tensor = torch.from_numpy(vectorized).to(used_device).to(torch.float32)\n",
    "    \n",
    "    # pad or truncate\n",
    "    vectorized_tensor = pad_or_truncate(vectorized_tensor, context_length)\n",
    "    \n",
    "    if flatten:\n",
    "        # flatten to fit into first fc layer of the net\n",
    "        vectorized_tensor = vectorized_tensor.flatten()\n",
    "    \n",
    "    return vectorized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_to_segment(start: np.ndarray, iterations: int, embeddings_model: Word2Vec, net: REAN, rand_seg_range: int=5) -> list:\n",
    "    start = torch.from_numpy(start).to(torch.float32).to(run_device).unsqueeze(0)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        start += net(start)\n",
    "    \n",
    "    return devectorize_sentence(start.detach().squeeze(0).cpu().numpy(), embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_lerp_steps(vectorized_sentence: np.ndarray, model: Word2Vec, steps: int) -> list[np.ndarray]:\n",
    "    random_point = np.random.uniform(np.min(vectorized_sentence), np.max(vectorized_sentence), (len(vectorized_sentence), vector_size))\n",
    "    \n",
    "    sentence_steps = []\n",
    "    \n",
    "    for indicies in range(steps):\n",
    "        sentence_steps.append(vectorized_sentence + (random_point - vectorized_sentence) * (indicies / steps))\n",
    "    \n",
    "    return sentence_steps[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN_dataset(Dataset):\n",
    "    def __init__(self, path, num_unique_examples, segment_size, embeddings_model, lerp_steps):\n",
    "        self.seek_start = 0\n",
    "        \n",
    "        self.segments = []\n",
    "        self.diffs = []\n",
    "        \n",
    "        for _ in tqdm(range(num_unique_examples)):\n",
    "            self.current_chunk, self.eof, self.seek_start = load_dataset_chunk(path, segment_size, self.seek_start)\n",
    "            \n",
    "            self.lerp_steps = get_sentence_lerp_steps(vectorize_sentence(self.current_chunk, embeddings_model), embeddings_model, lerp_steps)\n",
    "            \n",
    "            for self.current_step_idx in range(len(self.lerp_steps) - 1):\n",
    "                self.net_input = torch.from_numpy(self.lerp_steps[self.current_step_idx]).to(torch.float32).to(storage_device)\n",
    "                self.net_target = torch.from_numpy(self.lerp_steps[self.current_step_idx + 1]).to(torch.float32).to(storage_device)\n",
    "                \n",
    "                self.net_output = self.net_target - self.net_input\n",
    "                \n",
    "                self.segments.append(self.net_input)\n",
    "                self.diffs.append(self.net_output)\n",
    "            \n",
    "            if self.eof:\n",
    "                print(\"eof hit, early stop\")\n",
    "                print(f\"fluffed up size: {len(self.segments)}\")\n",
    "                return\n",
    "        \n",
    "        print(f\"fluffed up size: {len(self.segments)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.segments[index], self.diffs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = REAN_dataset(train_dataset_path, unique_examples_train, segment_size, embeddings_model, lerp_steps)\n",
    "test_dataset = REAN_dataset(test_dataset_path, unique_examples_test, segment_size, embeddings_model, lerp_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_graph = []\n",
    "test_loss_graph = []\n",
    "learning_rate_graph = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    epoch_train_loss = 0\n",
    "    for current_segment, target in train_loader:\n",
    "        # move batch to gpu\n",
    "        current_segment = current_segment.to(run_device)\n",
    "        target = target.to(run_device)\n",
    "        \n",
    "        # train batch\n",
    "        optimizer.zero_grad()\n",
    "        train_outputs = net(current_segment)\n",
    "        train_loss_value = loss(train_outputs, target)\n",
    "        train_loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect performance metrics\n",
    "        epoch_train_loss += train_loss_value.item()\n",
    "    \n",
    "    train_loss_graph.append(epoch_train_loss / len(train_loader))\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        epoch_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for test_current_segment, test_target in test_loader:\n",
    "                # move to gpu\n",
    "                test_current_segment = test_current_segment.to(run_device)\n",
    "                test_target = test_target.to(run_device)\n",
    "                \n",
    "                test_outputs = net(test_current_segment)\n",
    "                test_loss_value = loss(test_outputs, test_target)\n",
    "                epoch_test_loss += test_loss_value.item()\n",
    "                \n",
    "        test_loss_graph.append(epoch_test_loss / len(test_loader))\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Track the learning rate\n",
    "    learning_rate_graph.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Real-time plotting\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_loss_graph, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot testing loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(0, len(test_loss_graph) * 3, 3), test_loss_graph, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot learning rate\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(learning_rate_graph, label='Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), 'no_attention_mech.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_to_segment(np.random.uniform(-5, 5, (segment_size, vector_size)), 3, embeddings_model, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
