{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from tqdm import tqdm\n",
    "import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset settings\n",
    "dataset_path = fr\"./datasets/ultra_train.txt\"\n",
    "\n",
    "chunk_size = 1024# * 8 * 8\n",
    "corpus_size = 4\n",
    "\n",
    "# model hyperparams\n",
    "vector_size = 4096      # Dimensionality of the word vectors\n",
    "window = 10             # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 2           # Ignores all words with total frequency lower than this\n",
    "workers = 1             # Number of worker threads to train the model\n",
    "sg = 0                  # Training algorithm: 1 for skip-gram; 0 for CBOW\n",
    "hs = 0                  # If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "negative = 28           # If > 0, negative sampling will be used. The int for negative specifies how many \"noise words\" should be drawn\n",
    "epochs = 8              # Number of iterations (epochs) over the corpus\n",
    "alpha = 0.025           # The initial learning rate\n",
    "min_alpha = 0.0001      # The minimum learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "model = Word2Vec(\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg,\n",
    "    hs=hs,\n",
    "    negative=negative,\n",
    "    alpha=alpha,\n",
    "    min_alpha=min_alpha\n",
    ")\n",
    "\n",
    "#model = Word2Vec.load(fr\"./embedding_models/360vec_checkpt1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_corpus:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.size = 0\n",
    "        self.corpus_to_size = False\n",
    "        self.all_chunks_to_size = True\n",
    "    \n",
    "    def add_chunk(self, chunk):\n",
    "        self.corpus.append(chunk)\n",
    "        \n",
    "        self.size += 1\n",
    "        self.corpus_to_size = self.size == corpus_size\n",
    "        self.all_chunks_to_size = self.all_chunks_to_size and \"\".join(chunk) == chunk_size\n",
    "    \n",
    "    def corpus_ok(self):\n",
    "        return self.corpus_to_size and self.all_chunks_to_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for chunk in self.corpus:\n",
    "            yield chunk\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(read_start: int, chunk_size: int=chunk_size, path: str=dataset_path) -> tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Reads a certain number of characters from a file starting at the specified position.\n",
    "    \n",
    "    Args:\n",
    "        read_start (int): The position in the file to start reading from.\n",
    "        chunk_size (int, optional): The number of characters to read. Defaults to `chunk_size`.\n",
    "        path (str, optional): The path to the dataset text file. Defaults to `dataset_path`.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[str, bool]: A tuple containing the loaded chunk and a boolean indicating if EOF is hit.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        file.seek(read_start)\n",
    "        chunk = file.read(chunk_size)\n",
    "        \n",
    "        # eof hit check\n",
    "        if not chunk:\n",
    "            return \"\", True\n",
    "        \n",
    "        return chunk, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(read_start: int, corpus_size: int=corpus_size, chunk_size: int=chunk_size, path: str=dataset_path) -> tuple[training_corpus, bool]:\n",
    "    corpus = training_corpus()\n",
    "    \n",
    "    for _ in range(corpus_size):\n",
    "        chunk, eof = read_chunk(read_start, chunk_size, path)\n",
    "        \n",
    "        # check eof\n",
    "        if eof:\n",
    "            return corpus, True\n",
    "        \n",
    "        corpus.add_chunk(tokenizer.tokenize_segment(chunk))\n",
    "        \n",
    "        read_start += chunk_size\n",
    "        \n",
    "    return corpus, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_corpus(model: Word2Vec, corpus: training_corpus, first=False):\n",
    "    \"\"\"\n",
    "    Train a Word2Vec model on a given corpus chunk.\n",
    "\n",
    "    Args:\n",
    "        model (Word2Vec): The Word2Vec model to train.\n",
    "        corpus (list): A list of tokenized sentences to train on.\n",
    "        first (bool): If True, build the vocabulary from the corpus. Use this only for the first chunk.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if first:\n",
    "        model.build_vocab(corpus.corpus)\n",
    "        total_examples = model.corpus_count\n",
    "    else:\n",
    "        # Update the vocabulary with new words from the corpus\n",
    "        model.build_vocab(corpus.corpus, update=True)\n",
    "        total_examples = model.corpus_count\n",
    "    \n",
    "    model.train(corpus.corpus, total_examples=total_examples, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(model, dataset_path, train_corpuses):\n",
    "    read_start = 0\n",
    "    corpus = training_corpus()\n",
    "    \n",
    "    # first train\n",
    "    corpus, eof = build_corpus(read_start, corpus_size, chunk_size, dataset_path)\n",
    "    train_corpus(model, corpus, first=True)\n",
    "    \n",
    "    for _ in tqdm(range(train_corpuses)):\n",
    "        corpus, eof = build_corpus(read_start, corpus_size, chunk_size, dataset_path)\n",
    "        \n",
    "        if eof:\n",
    "            break\n",
    "        \n",
    "        if _ > 85858 or True:\n",
    "            train_corpus(model, corpus)\n",
    "        \n",
    "        read_start += corpus_size * chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/192000 [00:00<6:25:24,  8.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 2755/192000 [17:06<19:35:38,  2.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfull_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mfull_train\u001b[0;34m(model, dataset_path, train_corpuses)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _ \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m85858\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtrain_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m read_start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m corpus_size \u001b[38;5;241m*\u001b[39m chunk_size\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mtrain_corpus\u001b[0;34m(model, corpus, first)\u001b[0m\n\u001b[1;32m     16\u001b[0m     total_examples \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcorpus_count\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Update the vocabulary with new words from the corpus\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     total_examples \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcorpus_count\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(corpus\u001b[38;5;241m.\u001b[39mcorpus, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, epochs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/gensim/models/word2vec.py:495\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n\u001b[0;32m--> 495\u001b[0m report_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_raw_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_raw_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m report_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimate_memory(vocab_size\u001b[38;5;241m=\u001b[39mreport_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_retained_words\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_weights(update\u001b[38;5;241m=\u001b[39mupdate)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/gensim/models/word2vec.py:776\u001b[0m, in \u001b[0;36mWord2Vec.prepare_vocab\u001b[0;34m(self, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_binary_tree()\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative:\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;66;03m# build the table for drawing random words (for negative sampling)\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_cum_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m report_values\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/gensim/models/word2vec.py:840\u001b[0m, in \u001b[0;36mWord2Vec.make_cum_table\u001b[0;34m(self, domain)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vocab_size):\n\u001b[1;32m    839\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mget_vecattr(word_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 840\u001b[0m     train_words_pow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns_exponent)\n\u001b[1;32m    841\u001b[0m cumulative \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vocab_size):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_train(model, dataset_path, 6000 * 16 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark:\n",
    "# 40 min train   /   580 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' could', 0.9956420660018921),\n",
       " (' with', 0.9951943755149841),\n",
       " (' those', 0.9949793815612793),\n",
       " (' one', 0.9946724772453308),\n",
       " (' idea', 0.9944334030151367),\n",
       " (' fan', 0.9943879842758179),\n",
       " (' enjoy', 0.9936926960945129),\n",
       " (' make', 0.9914247989654541),\n",
       " (' most', 0.9898692965507507),\n",
       " (' broadcast', 0.9895931482315063)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\" when\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fr\"./embedding_models/4096vec_checkpt1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
