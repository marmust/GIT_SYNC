{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset settings\n",
    "dataset_path = fr\"C:\\Users\\user\\Desktop\\AUTOENCODER\\git_drop\\GIT_SYNC\\datasets\\clean_test_dset.txt\"\n",
    "\n",
    "chunk_size = 1024\n",
    "corpus_size = 8\n",
    "\n",
    "# model hyperparams\n",
    "vector_size = 50        # Dimensionality of the word vectors\n",
    "window = 3              # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 5           # Ignores all words with total frequency lower than this\n",
    "workers = 2             # Number of worker threads to train the model\n",
    "sg = 0                  # Training algorithm: 1 for skip-gram; 0 for CBOW\n",
    "hs = 0                  # If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "negative = 5            # If > 0, negative sampling will be used. The int for negative specifies how many \"noise words\" should be drawn\n",
    "epochs = 10             # Number of iterations (epochs) over the corpus\n",
    "alpha = 0.025           # The initial learning rate\n",
    "min_alpha = 0.0001      # The minimum learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "model = Word2Vec(\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg,\n",
    "    hs=hs,\n",
    "    negative=negative,\n",
    "    alpha=alpha,\n",
    "    min_alpha=min_alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_corpus:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.size = 0\n",
    "        self.corpus_to_size = False\n",
    "        self.all_chunks_to_size = True\n",
    "    \n",
    "    def add_chunk(self, chunk):\n",
    "        self.corpus.append(chunk)\n",
    "        \n",
    "        self.size += 1\n",
    "        self.corpus_to_size = self.size == corpus_size\n",
    "        self.all_chunks_to_size = self.all_chunks_to_size and \"\".join(chunk) == chunk_size\n",
    "    \n",
    "    def corpus_ok(self):\n",
    "        return self.corpus_to_size and self.all_chunks_to_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for chunk in self.corpus:\n",
    "            yield chunk\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(read_start: int, chunk_size: int=chunk_size, path: str=dataset_path) -> tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Reads a certain number of characters from a file starting at the specified position.\n",
    "    \n",
    "    Args:\n",
    "        read_start (int): The position in the file to start reading from.\n",
    "        chunk_size (int, optional): The number of characters to read. Defaults to `chunk_size`.\n",
    "        path (str, optional): The path to the dataset text file. Defaults to `dataset_path`.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[str, bool]: A tuple containing the loaded chunk and a boolean indicating if EOF is hit.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        file.seek(read_start)\n",
    "        chunk = file.read(chunk_size)\n",
    "        \n",
    "        # eof hit check\n",
    "        if not chunk:\n",
    "            return \"\", True\n",
    "        \n",
    "        return chunk, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(read_start: int, corpus_size: int=corpus_size, chunk_size: int=chunk_size, path: str=dataset_path) -> tuple[training_corpus, bool]:\n",
    "    corpus = training_corpus()\n",
    "    \n",
    "    for _ in range(corpus_size):\n",
    "        chunk, eof = read_chunk(read_start, chunk_size, path)\n",
    "        \n",
    "        # check eof\n",
    "        if eof:\n",
    "            return corpus, True\n",
    "        \n",
    "        corpus.add_chunk(simple_preprocess(chunk))\n",
    "        \n",
    "        read_start += chunk_size\n",
    "        \n",
    "    return corpus, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_corpus(model: Word2Vec, corpus: training_corpus, first=False):\n",
    "    \"\"\"\n",
    "    Train a Word2Vec model on a given corpus chunk.\n",
    "\n",
    "    Args:\n",
    "        model (Word2Vec): The Word2Vec model to train.\n",
    "        corpus (list): A list of tokenized sentences to train on.\n",
    "        first (bool): If True, build the vocabulary from the corpus. Use this only for the first chunk.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if first:\n",
    "        model.build_vocab(corpus.corpus)\n",
    "        total_examples = model.corpus_count\n",
    "    else:\n",
    "        total_examples = corpus.size\n",
    "    \n",
    "    model.train(corpus.corpus, total_examples=total_examples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(model, dataset_path, train_corpuses):\n",
    "    read_start = 0\n",
    "    corpus = training_corpus()\n",
    "    \n",
    "    # first train\n",
    "    corpus, eof = build_corpus(read_start, corpus_size, chunk_size, dataset_path)\n",
    "    train_corpus(model, corpus, first=True)\n",
    "    \n",
    "    for _ in tqdm(range(train_corpuses)):\n",
    "        corpus, eof = build_corpus(read_start, corpus_size, chunk_size, dataset_path)\n",
    "        \n",
    "        if eof:\n",
    "            break\n",
    "        \n",
    "        train_corpus(model, corpus)\n",
    "        \n",
    "        read_start += corpus_size * chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1024 [00:00<00:14, 70.41it/s]\n"
     ]
    }
   ],
   "source": [
    "full_train(model, dataset_path, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('your', 0.9985938668251038),\n",
       " ('been', 0.9985557794570923),\n",
       " ('of', 0.9985507130622864),\n",
       " ('my', 0.9985238313674927),\n",
       " ('it', 0.9984666109085083),\n",
       " ('in', 0.9984549880027771),\n",
       " ('its', 0.9984065890312195),\n",
       " ('do', 0.998383641242981),\n",
       " ('or', 0.9983698129653931),\n",
       " ('to', 0.9983481168746948)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
